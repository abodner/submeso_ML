{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on all timeseries removig JFM, test on JFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility\n",
    "\n",
    "BASE = '/scratch/ab10313/pleiades/'\n",
    "PATH_NN_surface = BASE+'NN_data_surface/'\n",
    "PATH_NN_interior = BASE+'NN_data_interior/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input shape:\n",
      "(8460, 3, 40, 40)\n",
      "\n",
      "Y output shape:\n",
      "(8460, 1, 40, 40)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load preprocessed data into input and output channels\n",
    "\n",
    "\n",
    "\n",
    "# X INPUT\n",
    "grad_B = np.load(PATH_NN_surface+'grad_B.npy')\n",
    "#FCOR = np.load(PATH_NN+'FCOR.npy')\n",
    "TAU = np.load(PATH_NN_surface+'TAU.npy')\n",
    "Q = np.load(PATH_NN_surface+'Q.npy')\n",
    "\n",
    "X_input = np.stack([ grad_B, TAU, Q],axis=1)\n",
    "print('X input shape:')\n",
    "print( X_input.shape)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Y OUTPUT\n",
    "WB_sg = np.load(PATH_NN_interior+'WB_sg.npy')\n",
    "WB_sg_mean = np.load(PATH_NN_interior+'WB_sg_mean.npy')\n",
    "WB_sg_std = np.load(PATH_NN_interior+'WB_sg_std.npy')\n",
    "              \n",
    "Y_output = np.tile(WB_sg,(1,1,1,1)).reshape(WB_sg.shape[0],1,WB_sg.shape[1],WB_sg.shape[2]) \n",
    "print('Y output shape:')\n",
    "print(Y_output.shape)\n",
    "print('')\n",
    "\n",
    "np.isnan(X_input).any()\n",
    "np.isnan(Y_output).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JFM & JAS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/ab10313/pleiades/15_bengal_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/12_agulhas_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/11_new_zealand_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/10_north_pacific_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/04_equator_atlantic_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/01_gulf_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/03_south_atlantic_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/05_argentina_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/13_australia_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/14_indian_ocean_smooth/preprcossed_data/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LIST_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Bengal','Agulhas', 'New Zealand', 'North Pacific', 'Equatorial Atlantic',\n",
    "              'Gulf', 'South Atlantic', 'Argentina', 'Australia', 'Indian Ocean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#season indecies\n",
    "JAS_ind_min = 577\n",
    "JAS_ind_max = 762\n",
    "\n",
    "JFM_ind_min = 213\n",
    "JFM_ind_max = 396\n",
    "\n",
    "#locations\n",
    "location_index = np.zeros(X_input.shape[0])\n",
    "JFM_index = np.empty(X_input.shape[0])\n",
    "JFM_index[:] = np.nan\n",
    "JAS_index = np.empty(X_input.shape[0])\n",
    "JAS_index[:] = np.nan\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(845):\n",
    "        location_index[i*845+j] = i\n",
    "\n",
    "        if JAS_ind_min<j & j<JAS_ind_max:\n",
    "            JAS_index[i*845+j] = i*845+j\n",
    "            \n",
    "        elif JFM_ind_min<j & j<JFM_ind_max:\n",
    "            JFM_index[i*845+j] = i*845+j\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate train set (not JFM), randomize, test set is JFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train: no JFM, test: JFM\n",
      "no overlapping indecies\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST ONLY\n",
    "# randomnly generate train, test and validation time indecies \n",
    "import random\n",
    "\n",
    "print(f\"Dataset: train: no JFM, test: JFM\")\n",
    "time_ind = np.arange(X_input.shape[0])\n",
    "train_ind = time_ind[np.isnan(JFM_index)]\n",
    "test_ind = time_ind[~np.isnan(JFM_index)]\n",
    "rand_seed = 14\n",
    "random.Random(rand_seed).shuffle(train_ind)\n",
    "\n",
    "# check no overlapping indecies\n",
    "if np.intersect1d(train_ind, test_ind).any():\n",
    "    print('overlapping indecies')\n",
    "else:\n",
    "    print ('no overlapping indecies')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defined train, test and val dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "X input shape:\n",
      "(6640, 3, 40, 40)\n",
      "Y output shape:\n",
      "(6640, 1, 40, 40)\n",
      "\n",
      "TEST\n",
      "X input shape:\n",
      "(1820, 3, 40, 40)\n",
      "Y output shape:\n",
      "(1820, 1, 40, 40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define X,Y pairs (state, subgrid fluxes) for local network.local_torch_dataset = Data.TensorDataset(\n",
    "BATCH_SIZE = 64  # Number of sample in each batch\n",
    "\n",
    "\n",
    "###### training dataset #######\n",
    "torch_dataset_train = Data.TensorDataset(\n",
    "    torch.from_numpy(X_input[train_ind]).double(),\n",
    "    torch.from_numpy(Y_output[train_ind]).double(),\n",
    ")\n",
    "\n",
    "loader_train = Data.DataLoader(\n",
    "    dataset=torch_dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "print('TRAIN')\n",
    "print('X input shape:')\n",
    "print( X_input[train_ind].shape)\n",
    "print('Y output shape:')\n",
    "print( Y_output[train_ind].shape)\n",
    "print('')\n",
    "\n",
    "###### test dataset #######\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_input[test_ind]).double(),\n",
    "    torch.from_numpy(Y_output[test_ind]).double(),    \n",
    ")\n",
    "\n",
    "BATCH_SIZE_TEST = len(torch_dataset_test)\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False\n",
    ")\n",
    "\n",
    "print('TEST')\n",
    "print('X input shape:')\n",
    "print( X_input[test_ind].shape)\n",
    "print('Y output shape:')\n",
    "print( Y_output[test_ind].shape)\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - direct fluxes 3 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "class CNN_direct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, 5, padding='same', padding_mode='reflect')  # 7 inputs, 128 neurons for first hidden layer\n",
    "        self.conv1_bn=nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 64, 5, padding='same', padding_mode='reflect')  # 128 inputs, 64 neurons for first hidden layer\n",
    "        self.conv2_bn=nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 48, 5, padding='same', padding_mode='reflect')  # 64 inputs, 32 neurons for first hidden layer\n",
    "        self.conv3_bn=nn.BatchNorm2d(48)  \n",
    "        self.conv4 = nn.Conv2d(48, 32, 5, padding='same', padding_mode='reflect')  # 32 inputs, 16 neurons for first hidden layer\n",
    "        self.conv4_bn=nn.BatchNorm2d(32) \n",
    "        self.conv5 = nn.Conv2d(32, 16, 5, padding='same', padding_mode='reflect')  # 64 inputs, 32 neurons for first hidden layer\n",
    "        self.conv5_bn=nn.BatchNorm2d(16)  \n",
    "        self.conv6 = nn.Conv2d(16, 8, 5, padding='same', padding_mode='reflect')  # 32 inputs, 16 neurons for first hidden layer\n",
    "        self.conv6_bn=nn.BatchNorm2d(8) \n",
    "        self.conv7 = nn.Conv2d(8, 1, 5, padding='same', padding_mode='reflect')  # 16 inputs, 1 neurons for first hidden layer\n",
    "        self.conv7_bn=nn.BatchNorm2d(1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.conv1_bn(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.conv2_bn(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.conv3_bn(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.conv4_bn(x))\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.conv5_bn(x))\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.conv6_bn(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv7_bn(x)\n",
    "        return x\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model: direct fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network \n",
    "def train_model_direct(net, criterion, trainloader, optimizer, len_train_dataset, text=\"train\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(trainloader)}\")\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        prediction = net(b_x.to(device))\n",
    "        loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "        nb = b_y.shape[0]\n",
    "        train_loss = train_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "    train_loss /= len_train_dataset  # dividing by the number of batches\n",
    "    print(text + \" loss:\", train_loss)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_model_direct(net, criterion, testloader, optimizer,len_test_dataset, text=\"test\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(testloader)}\")\n",
    "    \n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            testloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            prediction = net(b_x.to(device))\n",
    "            loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "            nb = b_y.shape[0]\n",
    "            test_loss = test_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "        test_loss /= len_test_dataset  # dividing by the number of batches\n",
    "        #         print(len(trainloader))\n",
    "        print(text + \" loss:\", test_loss)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_direct(net, criterion, testloader, len_test_dataset, text=\"test\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(testloader)}\")\n",
    "    \n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    #with torch.no_grad():\n",
    "    for step, (batch_x, batch_y) in enumerate(\n",
    "        testloader\n",
    "    ):  # for each training step\n",
    "        b_x = Variable(batch_x, requires_grad=True)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        prediction = net(b_x.to(device))\n",
    "        loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "        nb = b_y.shape[0]\n",
    "        test_loss = test_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "    test_loss /= len_test_dataset  # dividing by the number of batches\n",
    "    #         print(len(trainloader))\n",
    "    print(text + \" loss:\", test_loss)\n",
    "    return b_x, b_y, prediction\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # MSE loss function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.9610587476260485\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6795235667870382\n",
      "epoch: 2\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.8924779014559083\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6631687574592949\n",
      "epoch: 3\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.8618134708262856\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.70258623120482\n",
      "epoch: 4\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.8369063396821765\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6546052964208859\n",
      "epoch: 5\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.8048228821313889\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6249519217730868\n",
      "epoch: 6\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.7871362358843218\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6144371586458772\n",
      "epoch: 7\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.7698406348185834\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.664453183484298\n",
      "epoch: 8\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.7566957351645122\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.62363196261539\n",
      "epoch: 9\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.7389098140439581\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6317914999190286\n",
      "epoch: 10\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.730019128131485\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5723848379821324\n",
      "epoch: 11\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.7160434876464805\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.624139360116365\n",
      "epoch: 12\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.706641976507199\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5842515484575602\n",
      "epoch: 13\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6963745727616529\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6186026036876723\n",
      "epoch: 14\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6837958589833523\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5964317644392881\n",
      "epoch: 15\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6708005709449479\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.592703207768903\n",
      "epoch: 16\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6596904251441107\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5843222371071485\n",
      "epoch: 17\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6504809885732551\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6487036269282686\n",
      "epoch: 18\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.639912290661047\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6194462090362798\n",
      "epoch: 19\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6357088856021571\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6833534024842034\n",
      "epoch: 20\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6214937813586372\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5932772339790049\n",
      "epoch: 21\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.6038526618573172\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6276900726307206\n",
      "epoch: 22\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5958605940993478\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.597124929895104\n",
      "epoch: 23\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5793076104170514\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6124405774927226\n",
      "epoch: 24\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5767246355408842\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5909716398168737\n",
      "epoch: 25\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5634165709296737\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6055598199755348\n",
      "epoch: 26\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5567515660382278\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.7166101338868541\n",
      "epoch: 27\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5428395232200253\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5962365958158887\n",
      "epoch: 28\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5326472285674245\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6217139288926364\n",
      "epoch: 29\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5275660736260526\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6291621654156497\n",
      "epoch: 30\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5243187417235764\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.591376180969205\n",
      "epoch: 31\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5105079165437809\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6473711101082881\n",
      "epoch: 32\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.5005779077174808\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6502993662615527\n",
      "epoch: 33\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4980788135862547\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6036540408495648\n",
      "epoch: 34\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.49604682094604124\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6040360462540031\n",
      "epoch: 35\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4795020735041947\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.588547450122137\n",
      "epoch: 36\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4801421760371857\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6351253901683334\n",
      "epoch: 37\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4722445352292795\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6308368094346459\n",
      "epoch: 38\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4693576862233938\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.616373553812814\n",
      "epoch: 39\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.45825519921955504\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6456498918614841\n",
      "epoch: 40\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4499952171758865\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6432995835560341\n",
      "epoch: 41\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4442492576658493\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6342828055940133\n",
      "epoch: 42\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.44752476798973007\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.5861207023067785\n",
      "epoch: 43\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.43878619701684013\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6412161654071916\n",
      "epoch: 44\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.43443999250775917\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6054256766714412\n",
      "epoch: 45\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.42724370788393823\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6183559060501495\n",
      "epoch: 46\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4258682565452179\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6245511349139086\n",
      "epoch: 47\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4216713882135898\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6209047769374932\n",
      "epoch: 48\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4200511045563108\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.7689184213892248\n",
      "epoch: 49\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.41876176168036977\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6576205567649115\n",
      "epoch: 50\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.41345301967643633\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6089834320245598\n",
      "epoch: 51\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4117703376181037\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.607298078230021\n",
      "epoch: 52\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.40962254241914137\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.620132007059311\n",
      "epoch: 53\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.3999711080618537\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6252749452666898\n",
      "epoch: 54\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.4025032141501095\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6853085700328311\n",
      "epoch: 55\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.39875548516794224\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.648383514700214\n",
      "epoch: 56\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.39433523783928054\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.652425707216881\n",
      "epoch: 57\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n",
      "train loss: 0.3894008994685964\n",
      "Training starts on device Quadro RTX 8000, number of samples 1\n",
      "test loss: 0.6307721414761471\n",
      "epoch: 58\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/state/partition1/job-33017788/ipykernel_3450717/529046619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_submeso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_dataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_submeso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_dataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/job-33017788/ipykernel_3450717/2740547149.py\u001b[0m in \u001b[0;36mtrain_model_direct\u001b[0;34m(net, criterion, trainloader, optimizer, len_train_dataset, text)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculating loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "cnn_submeso = CNN_direct().double()\n",
    "\n",
    "n_epochs = 100  # Number of epocs could be increased\n",
    "optimizer = optim.Adam(cnn_submeso.parameters(), lr=0.1)\n",
    "test_loss = list()\n",
    "train_loss = list()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"epoch:\", epoch)\n",
    "    train_loss.append(train_model_direct(cnn_submeso, criterion, loader_train, optimizer,len(torch_dataset_train)))\n",
    "    test_loss.append(test_model_direct(cnn_submeso, criterion, loader_test, optimizer,len(torch_dataset_test)))\n",
    "\n",
    "torch.save(cnn_submeso, PATH_NN_surface+'cnn_7l_k5_surface_inputs_JFM_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss[1:], \"b\", linewidth=3,label=\"training loss\")\n",
    "plt.plot(test_loss[1:], \"lightblue\",linewidth=3, label=\"test loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_submeso = torch.load(PATH_NN_surface+'cnn_7l_k5_surface_inputs_JFM_test.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in cnn_submeso.parameters()\n",
    ")\n",
    "total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_x, b_y, prediction = plot_model_direct(cnn_submeso, criterion, loader_test,len(torch_dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobian_norm(x,y):\n",
    "  if y.shape[1] != 1:\n",
    "    print('Sorry')\n",
    "  \n",
    "  dydx = torch.zeros(x.shape[1])\n",
    "\n",
    "  grad = torch.autograd.grad(\n",
    "    outputs=y, inputs=x,\n",
    "    grad_outputs=torch.ones_like(y),\n",
    "    retain_graph=True, create_graph=True)[0]\n",
    "      \n",
    "  if grad.shape != x.shape:\n",
    "    print('Error in dimensions')\n",
    "\n",
    "  return torch.mean(grad**2, dim=[-2,-1]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = Jacobian_norm(b_x,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(10),jacobian.detach(),c=range(10),s=70,cmap='Dark2')\n",
    "my_xticks = ['FCOR', 'grad_B', 'HML', 'Nsquared', 'TAU', 'Q', 'HBL', 'div', 'vort', 'strain']\n",
    "plt.xticks(np.arange(10), my_xticks,rotation = 45);\n",
    "plt.title('sensitivity of prediction with respect to input field')\n",
    "plt.ylabel(r'$\\nabla_{input}\\ \\overline{wb}^z}$',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with FK\n",
    "WB_FK = np.load(PATH_NN+'WB_FK.npy')\n",
    "WB_FK_test = WB_FK[test_ind]\n",
    "\n",
    "# compare with FK_Lf\n",
    "Lf_FK11 = np.load(PATH_NN+'Lf_FK11.npy')\n",
    "Lf_FK11_test = Lf_FK11[test_ind]\n",
    "\n",
    "# compare with Bod_Lf\n",
    "Lf_BD23 = np.load(PATH_NN+'Lf_BD23.npy')\n",
    "Lf_BD23_test = Lf_BD23[test_ind]\n",
    "\n",
    "# renormalize using mean and std \n",
    "WB_sg_mean_test = WB_sg_mean[test_ind]\n",
    "WB_sg_std_test = WB_sg_std[test_ind] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "corr_pred_wb = stats.pearsonr(cnn_submeso(b_x.to(device)).detach().cpu().numpy().flatten(), b_y.flatten())\n",
    "\n",
    "print(\"correlation between prediction and target,  w'b':\",corr_pred_wb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "it =720\n",
    "\n",
    "\n",
    "mean_it = WB_sg_mean_test[it]\n",
    "std_it = WB_sg_std_test[it]\n",
    "\n",
    "plt.subplots(nrows=2, ncols=2, figsize=(12, 6))\n",
    "#fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.pcolor((cnn_submeso(b_x.to(device)).detach().cpu().numpy()[it,0,:,:])*std_it + mean_it,cmap='RdBu_r', vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ CNN prediction',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.pcolor((b_y[it,0,:,:])*std_it + mean_it,cmap='RdBu_r',vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ LLC4320',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.pcolor((0.07*25e3/np.maximum(Lf_FK11_test[it],5000))*WB_FK_test[it],cmap='RdBu_r',vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ FK11 param',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.pcolor((0.07*25e3/(Lf_BD23_test[it]))*WB_FK_test[it],cmap='RdBu_r',vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ Bodner23 param',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show\n",
    "#plt.savefig(BASE+'plots/cnn_prediction_snapshot_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_y.detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tile = np.transpose(np.tile(WB_sg_mean_test,(40,40,1,1)))\n",
    "std_tile = np.transpose(np.tile(WB_sg_std_test,(40,40,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "_=plt.hist(((0.07*17e3/np.maximum(Lf_FK11_test[:],5000))*WB_FK_test[:]).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"FK11 param\")\n",
    "_=plt.hist((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"LLC4320\")\n",
    "plt.xlim([-.5e-8,3.5e-8])\n",
    "plt.ylim([0,3e5])\n",
    "plt.legend()\n",
    "plt.xlabel('$\\overline{wb}^z$',fontsize=12)\n",
    "\n",
    "plt.subplot(132)\n",
    "_=plt.hist(((0.07*17e3/np.maximum(Lf_BD23_test[:],5000))*WB_FK_test[:]).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"Bodner23 param\")\n",
    "_=plt.hist((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"LLC4320\")\n",
    "plt.xlim([-.5e-8,3.5e-8])\n",
    "plt.ylim([0,3e5])\n",
    "plt.legend()\n",
    "plt.xlabel('$\\overline{wb}^z$',fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot(133)\n",
    "_=plt.hist((cnn_submeso(b_x.to(device)).detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"CNN prediction\")\n",
    "_=plt.hist((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"LLC4320\")\n",
    "plt.xlim([-.5e-8,3.5e-8])\n",
    "plt.ylim([0,3e5])\n",
    "plt.legend()\n",
    "plt.xlabel('$\\overline{wb}^z$',fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "#plt.savefig(BASE+'plots/cnn_prediction_hist.png')\n",
    "#plt.xlabel('$\\overline{wb}^z$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_da = xr.DataArray((b_y.detach().cpu().numpy()*std_tile + mean_tile), dims=[\"time\", \"axis\", \"i\",\"j\"])[:,0,:,:]\n",
    "param_FK11_da = xr.DataArray((0.07*25e3/np.maximum(Lf_FK11_test[:],5000))*WB_FK_test[:], dims=[\"time\", \"i\",\"j\"])\n",
    "param_BD23_da = xr.DataArray((0.07*25e3/np.maximum(Lf_BD23_test[:],5000))*WB_FK_test[:], dims=[\"time\", \"i\",\"j\"])\n",
    "prediction_da = xr.DataArray((cnn_submeso(b_x.to(device)).detach().cpu().numpy()*std_tile + mean_tile), dims=[\"time\", \"axis\", \"i\",\"j\"])[:,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xrft\n",
    "\n",
    "# target vs prediction spectra\n",
    "\n",
    "WsBs_target_spectra = xrft.isotropic_power_spectrum(target_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "WsBs_prediction_spectra = xrft.isotropic_power_spectrum(prediction_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "WsBs_param_FK11_spectra = xrft.isotropic_power_spectrum(param_FK11_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "WsBs_param_BD23_spectra = xrft.isotropic_power_spectrum(param_BD23_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 10))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(311)\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, 5e-3*WsBs_target_spectra.freq_r**-2, '--',linewidth=1.5,color='black', label='k^{-2}')\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, 2e-3*WsBs_target_spectra.freq_r**-3, '--',linewidth=1.5,color='gray', label='k^{-3}')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_target_spectra,linewidth=3, label='target')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_prediction_spectra,linewidth=3, label='prediction')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_param_FK11_spectra,linewidth=3, label='FK11')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_param_BD23_spectra,linewidth=3, label='Bodner23')\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_param_BD_spectra*WsBs_target_spectra.freq_r,linewidth=3, label='BD23')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('$k_r$')\n",
    "#plt.ylabel('Normalized isotropic spectra')\n",
    "plt.title('WsBs')\n",
    "#plt.ylim([1e-3,1e-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH_NN+'test_prediction_7L_k7.npy',cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,0,:,:])\n",
    "np.save(PATH_NN+'test_target_7L_k7.npy',b_y[:,0,:,:])\n",
    "np.save(PATH_NN+'test_WB_FK.npy',WB_FK_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = np.load(PATH_NN+'test_prediction_7L_k7.npy')\n",
    "test_target = np.load(PATH_NN+'test_target_7L_k7.npy')\n",
    "test_FK = np.load(PATH_NN+'test_WB_FK.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "it =2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.pcolor(test_prediction[it],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb prediction')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.pcolor(test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb target')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.pcolor(test_FK[it],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb FK')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "it =2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.pcolor(test_prediction[it]-test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb prediction')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.pcolor(test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb target')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.pcolor(test_FK[it]-test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb FK')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(test_target.flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"target\")\n",
    "_=plt.hist(test_prediction.flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"prediction\")\n",
    "_=plt.hist(WB_FK_test.flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"FK08\")\n",
    "\n",
    "\n",
    "plt.ylim([0,.3e6])\n",
    "plt.xlim([-3,3])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_FK11_param = xr.DataArray(((0.07*25e3/np.maximum(Lf_FK11_test[:],5000))*WB_FK_test[:]).flatten())\n",
    "WB_Bodner23_param = xr.DataArray(((0.07*25e3/np.maximum(Lf_BD23_test[:],5000))*WB_FK_test[:]).flatten())\n",
    "WB_sg = xr.DataArray((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten())\n",
    "WB_cnn = xr.DataArray((cnn_submeso(b_x.to(device)).detach().cpu().numpy()*std_tile + mean_tile).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_Bodner23_param.name = 'Bodner23'\n",
    "WB_FK11_param.name = 'FK11'\n",
    "WB_sg.name = 'WB_subgrid'\n",
    "WB_cnn.name = 'WB_cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhistogram.xarray import histogram as xhist\n",
    "\n",
    "xhist_Bodner23 = xhist(WB_Bodner23_param, WB_sg,\n",
    "                       bins=[np.logspace(-10,-6,30), \n",
    "                             np.concatenate((-np.logspace(-6,-10,30),\n",
    "                                             np.concatenate((np.linspace(-9e-11,9e-11,5),\n",
    "                                                             np.logspace(-10,-6,30)))))\n",
    "                            ]\n",
    "                      ).compute()\n",
    "\n",
    "xhist_FK11 = xhist(WB_FK11_param, \n",
    "      WB_sg,\n",
    "      bins=[np.logspace(-10,-6,30), \n",
    "            np.concatenate((-np.logspace(-6,-10,30),\n",
    "                            np.concatenate((np.linspace(-9e-11,9e-11,5),\n",
    "                                            np.logspace(-10,-6,30)))))\n",
    "           ]\n",
    "     ).compute()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xhist_CNN = xhist(WB_cnn, \n",
    "      WB_sg,\n",
    "      bins=[np.logspace(-10,-6,30), \n",
    "            np.concatenate((-np.logspace(-6,-10,30),\n",
    "                            np.concatenate((np.linspace(-9e-11,9e-11,5),\n",
    "                                            np.logspace(-10,-6,30)))))\n",
    "           ]\n",
    "     ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "xhist_Bodner23.plot(ax=ax, cmap='Blues')\n",
    "\n",
    "ax.plot(np.linspace(1e-10,1e-6), np.linspace(1e-10,1e-6), c='grey', ls='--')\n",
    "ax.plot(np.linspace(-1e-6,-1e-10), np.linspace(1e-6,1e-10), c='grey', ls='--')\n",
    "ax.set_xscale('symlog', linthresh=1e-10)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-10,1e-6])\n",
    "ax.set_xlim([-1e-6,1e-6])\n",
    "ax.set_xlabel(r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\", fontsize=13)\n",
    "ax.set_ylabel(r\"$\\Psi^{\\tt Bod}\\times\\overline{\\langle b^m}^z$\", \n",
    "              fontsize=13)\n",
    "\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb_medianCr-hist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "xhist_FK11.plot(ax=ax, cmap='Blues')\n",
    "\n",
    "ax.plot(np.linspace(1e-10,1e-6), np.linspace(1e-10,1e-6), c='grey', ls='--')\n",
    "ax.plot(np.linspace(-1e-6,-1e-10), np.linspace(1e-6,1e-10), c='grey', ls='--')\n",
    "ax.set_xscale('symlog', linthresh=1e-10)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-10,1e-6])\n",
    "ax.set_xlim([-1e-6,1e-6])\n",
    "ax.set_xlabel(r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\", fontsize=13)\n",
    "ax.set_ylabel(r\"$\\Psi^{\\tt FK}\\times\\overline{\\nabla b}^z$\", \n",
    "              fontsize=13)\n",
    "\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb_medianCr-hist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "xhist_CNN.plot(ax=ax, vmax=10000, cmap='Blues')\n",
    "\n",
    "ax.plot(np.linspace(1e-10,1e-6), np.linspace(1e-10,1e-6), c='grey', ls='--')\n",
    "ax.plot(np.linspace(-1e-6,-1e-10), np.linspace(1e-6,1e-10), c='grey', ls='--')\n",
    "ax.set_xscale('symlog', linthresh=1e-10)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-10,1e-6])\n",
    "ax.set_xlim([-1e-6,1e-6])\n",
    "ax.set_xlabel(r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\", fontsize=13)\n",
    "ax.set_ylabel(r\"$CNN prediction$\", \n",
    "              fontsize=13)\n",
    "\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb_medianCr-hist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "#xhist(WB_Bodner23_param, \n",
    "#      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "#                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "#                                            np.logspace(-11,-4,30)))))\n",
    "#           ]\n",
    "     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "xhist(WB_cnn, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"CNN\")\n",
    "xhist(WB_FK11_param, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "xhist(WB_sg, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\")\n",
    "ax.set_xscale('symlog', linthresh=1e-11)\n",
    "ax.set_xlabel(r\"[m$^2$ s$^{-3}$]\", fontsize=13)\n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb-1Dhist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "#xhist(WB_Bodner23_param, \n",
    "#      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "#                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "#                                            np.logspace(-11,-4,30)))))\n",
    "#           ]\n",
    "#     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "#xhist(WB_cnn, \n",
    "#      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "#                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "#                                            np.logspace(-11,-4,30)))))\n",
    "#           ]\n",
    "#     ).plot(ax=ax, label=r\"CNN\")\n",
    "xhist(WB_FK11_param, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "xhist(WB_sg, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\")\n",
    "ax.set_xscale('symlog', linthresh=1e-11)\n",
    "ax.set_xlabel(r\"[m$^2$ s$^{-3}$]\", fontsize=13)\n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb-1Dhist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
