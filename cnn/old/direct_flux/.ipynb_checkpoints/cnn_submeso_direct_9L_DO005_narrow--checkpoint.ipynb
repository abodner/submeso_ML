{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits datasets along the spacial axes and concats them back into single array under time\n",
    "\n",
    "def normalize(data):\n",
    "    #normalized_data =  (data - data.mean(skipna=True).values)/data.std(skipna=True).values\n",
    "    normalized_data = (data - np.mean(data))/np.std(data)\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def load_norm_data(var_name_string):\n",
    "    PATH_LIST = glob.glob(BASE+'*/preprocessed_data/'+var_name_string+'.nc') \n",
    "    data_app = normalize(xr.open_dataarray(PATH_LIST[0]).to_numpy()[:,5:-5,5:-5])\n",
    "    for i_file in np.arange(1,len(PATH_LIST)):\n",
    "        PATH = PATH_LIST[i_file]\n",
    "        data = xr.open_dataarray(PATH).to_numpy()[:,5:-5,5:-5]\n",
    "        data_norm = normalize(data)\n",
    "        data_app = np.concatenate((data_app,data_norm),axis=0)\n",
    "    return data_app\n",
    "\n",
    "# depth of CNN * (kernel_num-1)/2\n",
    "#def load_norm_data_out(var_name_string): #with cropping for output\n",
    "#    PATH_LIST = glob.glob(BASE+'*/preprocessed_data/'+var_name_string+'.nc') \n",
    "#    data_app = normalize(xr.open_dataarray(PATH_LIST[0]).to_numpy()[:,14:-14,14:-14])\n",
    "#    for i_file in np.arange(1,len(PATH_LIST)):\n",
    "#        PATH = PATH_LIST[i_file]\n",
    "#        data = xr.open_dataarray(PATH).to_numpy()[:,14:-14,14:-14]\n",
    "#        data_norm = normalize(data)\n",
    "#        data_app = np.concatenate((data_app,data_norm),axis=0)\n",
    "#    return data_app\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    data_split_halfs = np.concatenate((np.split(data,2,axis=1)[0],np.split(data,2,axis=1)[1]),0)\n",
    "    data_split_quarters = np.concatenate((np.split(data_split_halfs,2,axis=2)[0],np.split(data_split_halfs,2,axis=2)[1]),0)\n",
    "    return data_split_quarters\n",
    "\n",
    "\n",
    "\n",
    "def load_split_norm_data(var_name_string):\n",
    "    loaded_data = load_norm_data(var_name_string)\n",
    "    splitted_data = split_data(loaded_data)\n",
    "    return splitted_data\n",
    "    \n",
    "    \n",
    "def load_split_norm_data_out(var_name_string):\n",
    "    loaded_data = load_norm_data(var_name_string)\n",
    "    splitted_data = split_data(loaded_data)\n",
    "    cropped_output = splitted_data[:,5:-5,5:-5] # for a kernel of size 3 with 5 conv layers\n",
    "    return cropped_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data, convert to numpy and stack channels\n",
    "\n",
    "BASE = '/scratch/ab10313/submeso_ML_data/'\n",
    "FULL_PATH_PP = glob.glob(BASE+'*/preprocessed_data/')\n",
    "\n",
    "# X INPUT\n",
    "ds_Bm = load_split_norm_data('Bm_MLD_lowres')\n",
    "ds_Um = load_split_norm_data('Um_MLD_lowres')\n",
    "ds_Vm = load_split_norm_data('Vm_MLD_lowres')\n",
    "ds_Wm = load_split_norm_data('Wm_MLD_lowres')\n",
    "\n",
    "\n",
    "X_input = np.stack([ds_Bm,ds_Um,ds_Vm,ds_Wm],axis=1)\n",
    "print('X input shape:')\n",
    "print( X_input.shape)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Y OUTPUT\n",
    "ds_UsBs = load_split_norm_data('UsBs_MLD_lowres')\n",
    "ds_VsBs = load_split_norm_data('VsBs_MLD_lowres')\n",
    "ds_WsBs = load_split_norm_data('WsBs_MLD_lowres')\n",
    "\n",
    "Y_output = np.stack([ds_UsBs,ds_VsBs,ds_WsBs],axis=1)\n",
    "print('Y output shape:')\n",
    "print(Y_output.shape)\n",
    "print('')\n",
    "\n",
    "\n",
    "# GRAD B\n",
    "ds_Bm_x = load_split_norm_data('Bm_x_MLD_lowres')\n",
    "ds_Bm_y = load_split_norm_data('Bm_y_MLD_lowres')\n",
    "ds_Bm_z = load_split_norm_data('Bm_z_MLD_lowres')\n",
    "\n",
    "grad_b = np.stack([ds_Bm_x,ds_Bm_y,ds_Bm_z],axis=1)\n",
    "print('grad b shape:')\n",
    "print( grad_b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomly generate train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN AND TEST ONLY\n",
    "# randomnly generate train, test and validation time indecies \n",
    "import random\n",
    "time_ind = X_input.shape[0]\n",
    "rand_ind = np.arange(time_ind)\n",
    "rand_seed = 14\n",
    "random.Random(rand_seed).shuffle(rand_ind)\n",
    "train_percent = 0.9\n",
    "test_percent = 0.1 \n",
    "print(f\"Dataset: train {np.round(train_percent*100)}%, test {np.round(test_percent*100)}%\")\n",
    "train_ind, test_ind =  rand_ind[:round(train_percent*time_ind)], rand_ind[round((train_percent)*time_ind):]                                                                        \n",
    "\n",
    "# check no overlapping indecies\n",
    "if np.intersect1d(train_ind, test_ind).any():\n",
    "    print('overlapping indecies')\n",
    "else:\n",
    "    print ('no overlapping indecies')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defined train, test and val dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X,Y pairs (state, subgrid fluxes) for local network.local_torch_dataset = Data.TensorDataset(\n",
    "BATCH_SIZE = 64  # Number of sample in each batch\n",
    "\n",
    "\n",
    "###### training dataset #######\n",
    "torch_dataset_train = Data.TensorDataset(\n",
    "    torch.from_numpy(X_input[train_ind]).double(),\n",
    "    torch.from_numpy(Y_output[train_ind]).double(),\n",
    ")\n",
    "\n",
    "loader_train = Data.DataLoader(\n",
    "    dataset=torch_dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "print('TRAIN')\n",
    "print('X input shape:')\n",
    "print( X_input[train_ind].shape)\n",
    "print('Y output shape:')\n",
    "print( Y_output[train_ind].shape)\n",
    "print('')\n",
    "\n",
    "###### test dataset #######\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_input[test_ind]).double(),\n",
    "    torch.from_numpy(Y_output[test_ind]).double(),    \n",
    ")\n",
    "\n",
    "BATCH_SIZE_TEST = len(torch_dataset_test)\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=True\n",
    ")\n",
    "\n",
    "print('TEST')\n",
    "print('X input shape:')\n",
    "print( X_input[test_ind].shape)\n",
    "print('Y output shape:')\n",
    "print( Y_output[test_ind].shape)\n",
    "print('')\n",
    "\n",
    "###### validation dataset #######\n",
    "#torch_dataset_val = Data.TensorDataset(\n",
    "#    torch.from_numpy(X_input[val_ind]).double(),\n",
    "#    torch.from_numpy(Y_output[val_ind]).double(),\n",
    "#)\n",
    "\n",
    "\n",
    "#loader_val = Data.DataLoader(\n",
    "#    dataset=torch_dataset_val, batch_size=BATCH_SIZE, shuffle=True\n",
    "#)\n",
    "\n",
    "#print('VAL')\n",
    "#print('X input shape:')\n",
    "#print( X_input[val_ind].shape)\n",
    "#print('Y output shape:')\n",
    "#print( Y_output[val_ind].shape)\n",
    "#print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - direct fluxes 3 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "class CNN_direct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 128, 3, padding='same', padding_mode='reflect')  # 4 inputs, 64 neurons for first hidden layer\n",
    "        self.dropout1 = nn.Dropout(0.05)\n",
    "        self.conv1_bn=nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 96, 3, padding='same', padding_mode='reflect')  # 64 inputs, 32 neurons for first hidden layer\n",
    "        self.dropout2 = nn.Dropout(0.05)\n",
    "        self.conv2_bn=nn.BatchNorm2d(96)\n",
    "        self.conv3 = nn.Conv2d(96, 78, 3, padding='same', padding_mode='reflect')  # 32 inputs, 16 neurons for first hidden layer\n",
    "        self.dropout3 = nn.Dropout(0.05)\n",
    "        self.conv3_bn=nn.BatchNorm2d(78)          \n",
    "        self.conv4 = nn.Conv2d(78, 64, 3, padding='same', padding_mode='reflect')  # 32 inputs, 16 neurons for first hidden layer\n",
    "        self.dropout4 = nn.Dropout(0.05)\n",
    "        self.conv4_bn=nn.BatchNorm2d(64)  \n",
    "        self.conv5 = nn.Conv2d(64, 32, 3, padding='same', padding_mode='reflect')  # 16 inputs, 3 neurons for first hidden layer\n",
    "        self.dropout5 = nn.Dropout(0.05)\n",
    "        self.conv5_bn=nn.BatchNorm2d(32) \n",
    "        self.conv6 = nn.Conv2d(32,24, 3, padding='same', padding_mode='reflect')  # 32 inputs, 24 neurons for first hidden layer\n",
    "        self.dropout6 = nn.Dropout(0.05)\n",
    "        self.conv6_bn=nn.BatchNorm2d(24) # 9 outputs\n",
    "        self.conv7 = nn.Conv2d(24, 16, 3, padding='same', padding_mode='reflect')  # 32 inputs, 24 neurons for first hidden layer\n",
    "        self.dropout7 = nn.Dropout(0.05)\n",
    "        self.conv7_bn=nn.BatchNorm2d(16) # 3 outputs\n",
    "        self.conv8 = nn.Conv2d(16, 9, 3, padding='same', padding_mode='reflect')  # 32 inputs, 24 neurons for first hidden layer\n",
    "        self.dropout8 = nn.Dropout(0.05)\n",
    "        self.conv8_bn=nn.BatchNorm2d(9) # 3 outputs\n",
    "        self.conv9 = nn.Conv2d(9, 3, 3, padding='same', padding_mode='reflect')  # 32 inputs, 24 neurons for first hidden layer\n",
    "        self.dropout9 = nn.Dropout(0.05)\n",
    "        self.conv9_bn=nn.BatchNorm2d(3) # 3 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.conv1_bn(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.conv2_bn(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.conv3_bn(x))\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = F.relu(self.conv4_bn(x))\n",
    "        x = self.conv5(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = F.relu(self.conv5_bn(x))\n",
    "        x = self.conv6(x)\n",
    "        x = self.dropout6(x)\n",
    "        x = F.relu(self.conv6_bn(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.dropout7(x)\n",
    "        x = F.relu(self.conv7_bn(x))\n",
    "        x = self.conv8(x)\n",
    "        x = self.dropout8(x)\n",
    "        x = F.relu(self.conv8_bn(x))\n",
    "        x = self.conv9(x)\n",
    "        x = self.dropout9(x)\n",
    "        x = self.conv9_bn(x)\n",
    "        return x\n",
    " \n",
    "    \n",
    "# define network structure in pytorch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "#class CNN_direct(nn.Module):\n",
    "#    def __init__(self):\n",
    "#        super().__init__()\n",
    "#        self.conv1 = nn.Conv2d(4, 64, 5,padding='same')  # 4 inputs, 128 neurons for first hidden layer\n",
    "#        self.conv2 = nn.Conv2d(64, 32, 5,padding='same')  # 64 inputs, 32 neurons for first hidden layer\n",
    "#        self.conv3 = nn.Conv2d(32, 3, 5,padding='same')  # 32 inputs, 24 neurons for first hidden layer\n",
    "\n",
    "#    def forward(self, x):\n",
    "#        x = FF.relu(self.conv1(x))\n",
    "#        x = FF.relu(self.conv2(x))\n",
    "#        x = self.conv3(x)\n",
    "#        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model: direct fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network \n",
    "def train_model_direct(net, criterion, trainloader, optimizer, len_train_dataset, text=\"train\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(trainloader)}\")\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        prediction = net(b_x.to(device))\n",
    "        loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "        nb = b_y.shape[0]\n",
    "        train_loss = train_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "    train_loss /= len_train_dataset  # dividing by the number of batches\n",
    "    print(text + \" loss:\", train_loss)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_model_direct(net, criterion, testloader, optimizer,len_test_dataset, text=\"test\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(testloader)}\")\n",
    "    \n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            testloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            prediction = net(b_x.to(device))\n",
    "            loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "            nb = b_y.shape[0]\n",
    "            test_loss = test_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "        test_loss /= len_test_dataset  # dividing by the number of batches\n",
    "        #         print(len(trainloader))\n",
    "        print(text + \" loss:\", test_loss)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_direct(net, criterion, testloader, optimizer, len_test_dataset, text=\"test\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(testloader)}\")\n",
    "    \n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            testloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            prediction = net(b_x.to(device))\n",
    "            loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "            nb = b_y.shape[0]\n",
    "            test_loss = test_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "        test_loss /= len_test_dataset  # dividing by the number of batches\n",
    "        #         print(len(trainloader))\n",
    "        print(text + \" loss:\", test_loss)\n",
    "    return b_x, b_y, prediction\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # MSE loss function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "cnn_submeso = CNN_direct().double()\n",
    "\n",
    "n_epochs = 100  # Number of epocs could be increased\n",
    "optimizer = optim.Adam(cnn_submeso.parameters(), lr=0.03)\n",
    "test_loss = list()\n",
    "train_loss = list()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"epoch:\", epoch)\n",
    "    train_loss.append(train_model_direct(cnn_submeso, criterion, loader_train, optimizer,len(torch_dataset_train)))\n",
    "    test_loss.append(test_model_direct(cnn_submeso, criterion, loader_test, optimizer,len(torch_dataset_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss[1:], \"b\", label=\"training loss\")\n",
    "plt.plot(test_loss[1:], \"r\", label=\"test loss\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_x, b_y, prediction = plot_model_direct(cnn_submeso, criterion, loader_test, optimizer,len(torch_dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "corr_pred_ub = stats.pearsonr(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,0,:,:].flatten(), b_y[:,0,:,:].flatten())\n",
    "corr_pred_vb = stats.pearsonr(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,1,:,:].flatten(), b_y[:,1,:,:].flatten())\n",
    "corr_pred_wb = stats.pearsonr(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,2,:,:].flatten(), b_y[:,2,:,:].flatten())\n",
    "\n",
    "print(\"correlation between prediction and target,  u'b':\",corr_pred_ub )\n",
    "print(\"correlation between prediction and target,  v'b':\",corr_pred_vb )\n",
    "print(\"correlation between prediction and target,  w'b':\",corr_pred_wb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "it =30\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(8, 4))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.pcolor(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[it,0,:,:],cmap='viridis',vmin=-1,vmax =1)\n",
    "plt.title('ub prediction')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.pcolor(b_y[it,0,:,:],cmap='viridis',vmin=-1,vmax =1)\n",
    "plt.title('ub target')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.pcolor(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[it,1,:,:],cmap='viridis',vmin=-1,vmax =1)\n",
    "plt.title('vb prediction')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.pcolor(b_y[it,1,:,:],cmap='viridis',vmin=-1,vmax =1)\n",
    "plt.title('vb target')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.pcolor(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[it,2,:,:],cmap='viridis',vmin=-1,vmax =1)\n",
    "plt.title('wb prediction')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.pcolor(b_y[it,2,:,:],cmap='viridis',vmin=-1,vmax =1)\n",
    "plt.title('wb target')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(b_y.detach().cpu().numpy().flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"target\")\n",
    "_=plt.hist(cnn_submeso(b_x.to(device)).detach().cpu().numpy().flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"prediction\")\n",
    "\n",
    "plt.xlim([-3,3])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(131)\n",
    "_=plt.hist(b_y.detach().cpu().numpy()[:,0,:,:].flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"target\")\n",
    "_=plt.hist(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,0,:,:].flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"prediction\")\n",
    "plt.xlim([-3,3])\n",
    "plt.legend()\n",
    "plt.title('ub')\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "_=plt.hist(b_y.detach().cpu().numpy()[:,1,:,:].flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"target\")\n",
    "_=plt.hist(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,1,:,:].flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"prediction\")\n",
    "plt.xlim([-3,3])\n",
    "plt.legend()\n",
    "plt.title('vb')\n",
    "\n",
    "plt.subplot(133)\n",
    "_=plt.hist(b_y.detach().cpu().numpy()[:,2,:,:].flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"target\")\n",
    "_=plt.hist(cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,2,:,:].flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"prediction\")\n",
    "plt.xlim([-3,3])\n",
    "plt.legend()\n",
    "plt.title('wb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_da = xr.DataArray(b_y.detach().cpu().numpy(), dims=[\"time\", \"axis\", \"i\",\"j\"])\n",
    "prediction_da = xr.DataArray(cnn_submeso(b_x.to(device)).detach().cpu().numpy(), dims=[\"time\", \"axis\", \"i\",\"j\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xrft\n",
    "\n",
    "# target vs prediction spectra\n",
    "UsBs_target_spectra = xrft.isotropic_power_spectrum(target_da.isel(axis=0),dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "UsBs_prediction_spectra = xrft.isotropic_power_spectrum(prediction_da.isel(axis=0),dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "\n",
    "VsBs_target_spectra = xrft.isotropic_power_spectrum(target_da.isel(axis=1),dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "VsBs_prediction_spectra = xrft.isotropic_power_spectrum(prediction_da.isel(axis=1),dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "\n",
    "WsBs_target_spectra = xrft.isotropic_power_spectrum(target_da.isel(axis=2),dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "WsBs_prediction_spectra = xrft.isotropic_power_spectrum(prediction_da.isel(axis=2),dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(131)\n",
    "#plt.loglog(UsBs_target_spectra.freq_r*2*np.pi, 5e-3*UsBs_target_spectra.freq_r**-2, '--',linewidth=1.5,color='black', label='k^{-2}')\n",
    "#plt.loglog(UsBs_target_spectra.freq_r*2*np.pi, 2e-3*UsBs_target_spectra.freq_r**-3, '--',linewidth=1.5,color='gray', label='k^{-3}')\n",
    "plt.loglog(UsBs_target_spectra.freq_r*2*np.pi, UsBs_target_spectra/UsBs_target_spectra[0],linewidth=2, label='target')\n",
    "plt.loglog(UsBs_prediction_spectra.freq_r*2*np.pi, UsBs_prediction_spectra/UsBs_target_spectra[0],linewidth=2, label='prediction')\n",
    "plt.legend()\n",
    "plt.xlabel('$k_r$')\n",
    "plt.ylabel('Normalized isotropic spectra')\n",
    "plt.title('UsBs')\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "#plt.loglog(VsBs_target_spectra.freq_r*2*np.pi, 5e-3*VsBs_target_spectra.freq_r**-2, '--',linewidth=1.5,color='black', label='k^{-2}')\n",
    "#plt.loglog(VsBs_target_spectra.freq_r*2*np.pi, 2e-3*VsBs_target_spectra.freq_r**-3, '--',linewidth=1.5,color='gray', label='k^{-3}')\n",
    "plt.loglog(VsBs_target_spectra.freq_r*2*np.pi, VsBs_target_spectra/VsBs_target_spectra[0],linewidth=2, label='target')\n",
    "plt.loglog(VsBs_prediction_spectra.freq_r*2*np.pi, VsBs_prediction_spectra/VsBs_target_spectra[0],linewidth=2, label='prediction')\n",
    "plt.legend()\n",
    "plt.xlabel('$k_r$')\n",
    "#plt.ylabel('Normalized isotropic spectra')\n",
    "plt.title('VsBs')\n",
    "\n",
    "\n",
    "plt.subplot(133)\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, 5e-3*WsBs_target_spectra.freq_r**-2, '--',linewidth=1.5,color='black', label='k^{-2}')\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, 2e-3*WsBs_target_spectra.freq_r**-3, '--',linewidth=1.5,color='gray', label='k^{-3}')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_target_spectra/WsBs_target_spectra[0],linewidth=2, label='target')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_prediction_spectra/WsBs_prediction_spectra[0],linewidth=2, label='prediction')\n",
    "plt.legend()\n",
    "plt.xlabel('$k_r$')\n",
    "#plt.ylabel('Normalized isotropic spectra')\n",
    "plt.title('WsBs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
