{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on all timeseries removig JFM, test on JFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(14)  # For reproducibility\n",
    "torch.manual_seed(14)  # For reproducibility\n",
    "\n",
    "BASE = '/scratch/ab10313/pleiades/'\n",
    "PATH_NN_surface = BASE+'NN_data_surface/'\n",
    "PATH_NN_interior = BASE+'NN_data_interior/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input and output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input shape:\n",
      "(8460, 3, 40, 40)\n",
      "\n",
      "Y output shape:\n",
      "(8460, 1, 40, 40)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load preprocessed data into input and output channels\n",
    "\n",
    "\n",
    "\n",
    "# X INPUT\n",
    "grad_B = np.load(PATH_NN_surface+'grad_B.npy')\n",
    "#FCOR = np.load(PATH_NN+'FCOR.npy')\n",
    "TAU = np.load(PATH_NN_surface+'TAU.npy')\n",
    "Q = np.load(PATH_NN_surface+'Q.npy')\n",
    "\n",
    "X_input = np.stack([ grad_B, TAU, Q],axis=1)\n",
    "print('X input shape:')\n",
    "print( X_input.shape)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Y OUTPUT\n",
    "WB_sg = np.load(PATH_NN_interior+'WB_sg.npy')\n",
    "WB_sg_mean = np.load(PATH_NN_interior+'WB_sg_mean.npy')\n",
    "WB_sg_std = np.load(PATH_NN_interior+'WB_sg_std.npy')\n",
    "              \n",
    "Y_output = np.tile(WB_sg,(1,1,1,1)).reshape(WB_sg.shape[0],1,WB_sg.shape[1],WB_sg.shape[2]) \n",
    "print('Y output shape:')\n",
    "print(Y_output.shape)\n",
    "print('')\n",
    "\n",
    "np.isnan(X_input).any()\n",
    "np.isnan(Y_output).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JFM & JAS index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/ab10313/pleiades/15_bengal_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/12_agulhas_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/11_new_zealand_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/10_north_pacific_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/04_equator_atlantic_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/01_gulf_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/03_south_atlantic_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/05_argentina_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/13_australia_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/14_indian_ocean_smooth/preprcossed_data/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LIST_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Bengal','Agulhas', 'New Zealand', 'North Pacific', 'Equatorial Atlantic',\n",
    "              'Gulf', 'South Atlantic', 'Argentina', 'Australia', 'Indian Ocean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#season indecies\n",
    "JAS_ind_min = 577\n",
    "JAS_ind_max = 762\n",
    "\n",
    "JFM_ind_min = 213\n",
    "JFM_ind_max = 396\n",
    "\n",
    "#locations\n",
    "location_index = np.zeros(X_input.shape[0])\n",
    "JFM_index = np.empty(X_input.shape[0])\n",
    "JFM_index[:] = np.nan\n",
    "JAS_index = np.empty(X_input.shape[0])\n",
    "JAS_index[:] = np.nan\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(845):\n",
    "        location_index[i*845+j] = i\n",
    "\n",
    "        if JAS_ind_min<j & j<JAS_ind_max:\n",
    "            JAS_index[i*845+j] = i*845+j\n",
    "            \n",
    "        elif JFM_ind_min<j & j<JFM_ind_max:\n",
    "            JFM_index[i*845+j] = i*845+j\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate train set (not JFM), randomize, test set is JFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train: no JFM, test: JFM\n",
      "no overlapping indecies\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST ONLY\n",
    "# randomnly generate train, test and validation time indecies \n",
    "import random\n",
    "\n",
    "print(f\"Dataset: train: no JFM, test: JFM\")\n",
    "time_ind = np.arange(X_input.shape[0])\n",
    "train_ind = time_ind[np.isnan(JFM_index)]\n",
    "test_ind = time_ind[~np.isnan(JFM_index)]\n",
    "rand_seed = 14\n",
    "random.Random(rand_seed).shuffle(train_ind)\n",
    "\n",
    "# check no overlapping indecies\n",
    "if np.intersect1d(train_ind, test_ind).any():\n",
    "    print('overlapping indecies')\n",
    "else:\n",
    "    print ('no overlapping indecies')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defined train, test and val dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "X input shape:\n",
      "(6640, 3, 40, 40)\n",
      "Y output shape:\n",
      "(6640, 1, 40, 40)\n",
      "\n",
      "TEST\n",
      "X input shape:\n",
      "(1820, 3, 40, 40)\n",
      "Y output shape:\n",
      "(1820, 1, 40, 40)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define X,Y pairs (state, subgrid fluxes) for local network.local_torch_dataset = Data.TensorDataset(\n",
    "BATCH_SIZE = 64  # Number of sample in each batch\n",
    "\n",
    "\n",
    "###### training dataset #######\n",
    "torch_dataset_train = Data.TensorDataset(\n",
    "    torch.from_numpy(X_input[train_ind]).double(),\n",
    "    torch.from_numpy(Y_output[train_ind]).double(),\n",
    ")\n",
    "\n",
    "loader_train = Data.DataLoader(\n",
    "    dataset=torch_dataset_train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "print('TRAIN')\n",
    "print('X input shape:')\n",
    "print( X_input[train_ind].shape)\n",
    "print('Y output shape:')\n",
    "print( Y_output[train_ind].shape)\n",
    "print('')\n",
    "\n",
    "###### test dataset #######\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.from_numpy(X_input[test_ind]).double(),\n",
    "    torch.from_numpy(Y_output[test_ind]).double(),    \n",
    ")\n",
    "\n",
    "BATCH_SIZE_TEST = len(torch_dataset_test)\n",
    "\n",
    "loader_test = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False\n",
    ")\n",
    "\n",
    "print('TEST')\n",
    "print('X input shape:')\n",
    "print( X_input[test_ind].shape)\n",
    "print('Y output shape:')\n",
    "print( Y_output[test_ind].shape)\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - direct fluxes 3 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network structure in pytorch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "class CNN_direct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, 5, padding='same', padding_mode='reflect')  # 7 inputs, 128 neurons for first hidden layer\n",
    "        self.conv1_bn=nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 64, 5, padding='same', padding_mode='reflect')  # 128 inputs, 64 neurons for first hidden layer\n",
    "        self.conv2_bn=nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 48, 5, padding='same', padding_mode='reflect')  # 64 inputs, 32 neurons for first hidden layer\n",
    "        self.conv3_bn=nn.BatchNorm2d(48)  \n",
    "        self.conv4 = nn.Conv2d(48, 32, 5, padding='same', padding_mode='reflect')  # 32 inputs, 16 neurons for first hidden layer\n",
    "        self.conv4_bn=nn.BatchNorm2d(32) \n",
    "        self.conv5 = nn.Conv2d(32, 16, 5, padding='same', padding_mode='reflect')  # 64 inputs, 32 neurons for first hidden layer\n",
    "        self.conv5_bn=nn.BatchNorm2d(16)  \n",
    "        self.conv6 = nn.Conv2d(16, 8, 5, padding='same', padding_mode='reflect')  # 32 inputs, 16 neurons for first hidden layer\n",
    "        self.conv6_bn=nn.BatchNorm2d(8) \n",
    "        self.conv7 = nn.Conv2d(8, 1, 5, padding='same', padding_mode='reflect')  # 16 inputs, 1 neurons for first hidden layer\n",
    "        self.conv7_bn=nn.BatchNorm2d(1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.conv1_bn(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.conv2_bn(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.conv3_bn(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.conv4_bn(x))\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(self.conv5_bn(x))\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(self.conv6_bn(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv7_bn(x)\n",
    "        return x\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test model: direct fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network \n",
    "def train_model_direct(net, criterion, trainloader, optimizer, len_train_dataset, text=\"train\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(trainloader)}\")\n",
    "    \n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    for step, (batch_x, batch_y) in enumerate(trainloader):  # for each training step\n",
    "        b_x = Variable(batch_x)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        prediction = net(b_x.to(device))\n",
    "        loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "        optimizer.zero_grad()  # clear gradients for next train\n",
    "        loss.backward()  # backpropagation, compute gradients\n",
    "        optimizer.step()  # apply gradients to update weights\n",
    "        nb = b_y.shape[0]\n",
    "        train_loss = train_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "    train_loss /= len_train_dataset  # dividing by the number of batches\n",
    "    print(text + \" loss:\", train_loss)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_model_direct(net, criterion, testloader, optimizer,len_test_dataset, text=\"test\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(testloader)}\")\n",
    "    \n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, (batch_x, batch_y) in enumerate(\n",
    "            testloader\n",
    "        ):  # for each training step\n",
    "            b_x = Variable(batch_x)  # Inputs\n",
    "            b_y = Variable(batch_y)  # outputs\n",
    "            prediction = net(b_x.to(device))\n",
    "            loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "            nb = b_y.shape[0]\n",
    "            test_loss = test_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "        test_loss /= len_test_dataset  # dividing by the number of batches\n",
    "        #         print(len(trainloader))\n",
    "        print(text + \" loss:\", test_loss)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_direct(net, criterion, testloader, len_test_dataset, text=\"test\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net.to(device)\n",
    "    device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training starts on device {device_name}, number of samples {len(testloader)}\")\n",
    "    \n",
    "    net.eval()  # Evaluation mode (important when having dropout layers)\n",
    "    test_loss = 0\n",
    "    #with torch.no_grad():\n",
    "    for step, (batch_x, batch_y) in enumerate(\n",
    "        testloader\n",
    "    ):  # for each training step\n",
    "        b_x = Variable(batch_x, requires_grad=True)  # Inputs\n",
    "        b_y = Variable(batch_y)  # outputs\n",
    "        prediction = net(b_x.to(device))\n",
    "        loss = criterion(prediction, b_y.to(device))  # Calculating loss\n",
    "        nb = b_y.shape[0]\n",
    "        test_loss = test_loss + nb * loss.detach().cpu().numpy()  # Keep track of the loss\n",
    "    test_loss /= len_test_dataset  # dividing by the number of batches\n",
    "    #         print(len(trainloader))\n",
    "    print(text + \" loss:\", test_loss)\n",
    "    return b_x, b_y, prediction\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()  # MSE loss function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Training starts on device Quadro RTX 8000, number of samples 104\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(14)  # For reproducibility\n",
    "cnn_submeso = CNN_direct().double()\n",
    "\n",
    "n_epochs = 100  # Number of epocs could be increased\n",
    "optimizer = optim.Adam(cnn_submeso.parameters(), lr=0.1)\n",
    "test_loss = list()\n",
    "train_loss = list()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"epoch:\", epoch)\n",
    "    train_loss.append(train_model_direct(cnn_submeso, criterion, loader_train, optimizer,len(torch_dataset_train)))\n",
    "    test_loss.append(test_model_direct(cnn_submeso, criterion, loader_test, optimizer,len(torch_dataset_test)))\n",
    "\n",
    "torch.save(cnn_submeso, PATH_NN_surface+'cnn_7l_k5_surface_inputs_JFM_test.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss[1:], \"b\", linewidth=3,label=\"training loss\")\n",
    "plt.plot(test_loss[1:], \"lightblue\",linewidth=3, label=\"test loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_submeso = torch.load(PATH_NN_surface+'cnn_7l_k5_surface_inputs_JFM_test.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in cnn_submeso.parameters()\n",
    ")\n",
    "total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_x, b_y, prediction = plot_model_direct(cnn_submeso, criterion, loader_test,len(torch_dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobian_norm(x,y):\n",
    "  if y.shape[1] != 1:\n",
    "    print('Sorry')\n",
    "  \n",
    "  dydx = torch.zeros(x.shape[1])\n",
    "\n",
    "  grad = torch.autograd.grad(\n",
    "    outputs=y, inputs=x,\n",
    "    grad_outputs=torch.ones_like(y),\n",
    "    retain_graph=True, create_graph=True)[0]\n",
    "      \n",
    "  if grad.shape != x.shape:\n",
    "    print('Error in dimensions')\n",
    "\n",
    "  return torch.mean(grad**2, dim=[-2,-1]).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian = Jacobian_norm(b_x,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(10),jacobian.detach(),c=range(10),s=70,cmap='Dark2')\n",
    "my_xticks = ['FCOR', 'grad_B', 'HML', 'Nsquared', 'TAU', 'Q', 'HBL', 'div', 'vort', 'strain']\n",
    "plt.xticks(np.arange(10), my_xticks,rotation = 45);\n",
    "plt.title('sensitivity of prediction with respect to input field')\n",
    "plt.ylabel(r'$\\nabla_{input}\\ \\overline{wb}^z}$',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with FK\n",
    "WB_FK = np.load(PATH_NN+'WB_FK.npy')\n",
    "WB_FK_test = WB_FK[test_ind]\n",
    "\n",
    "# compare with FK_Lf\n",
    "Lf_FK11 = np.load(PATH_NN+'Lf_FK11.npy')\n",
    "Lf_FK11_test = Lf_FK11[test_ind]\n",
    "\n",
    "# compare with Bod_Lf\n",
    "Lf_BD23 = np.load(PATH_NN+'Lf_BD23.npy')\n",
    "Lf_BD23_test = Lf_BD23[test_ind]\n",
    "\n",
    "# renormalize using mean and std \n",
    "WB_sg_mean_test = WB_sg_mean[test_ind]\n",
    "WB_sg_std_test = WB_sg_std[test_ind] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "corr_pred_wb = stats.pearsonr(cnn_submeso(b_x.to(device)).detach().cpu().numpy().flatten(), b_y.flatten())\n",
    "\n",
    "print(\"correlation between prediction and target,  w'b':\",corr_pred_wb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "it =720\n",
    "\n",
    "\n",
    "mean_it = WB_sg_mean_test[it]\n",
    "std_it = WB_sg_std_test[it]\n",
    "\n",
    "plt.subplots(nrows=2, ncols=2, figsize=(12, 6))\n",
    "#fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.pcolor((cnn_submeso(b_x.to(device)).detach().cpu().numpy()[it,0,:,:])*std_it + mean_it,cmap='RdBu_r', vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ CNN prediction',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.pcolor((b_y[it,0,:,:])*std_it + mean_it,cmap='RdBu_r',vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ LLC4320',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.pcolor((0.07*25e3/np.maximum(Lf_FK11_test[it],5000))*WB_FK_test[it],cmap='RdBu_r',vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ FK11 param',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.pcolor((0.07*25e3/(Lf_BD23_test[it]))*WB_FK_test[it],cmap='RdBu_r',vmin=-1e-8, vmax=1e-8)\n",
    "plt.title('$\\overline{wb}^z$ Bodner23 param',fontsize = 12)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show\n",
    "#plt.savefig(BASE+'plots/cnn_prediction_snapshot_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_y.detach().cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tile = np.transpose(np.tile(WB_sg_mean_test,(40,40,1,1)))\n",
    "std_tile = np.transpose(np.tile(WB_sg_std_test,(40,40,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "_=plt.hist(((0.07*17e3/np.maximum(Lf_FK11_test[:],5000))*WB_FK_test[:]).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"FK11 param\")\n",
    "_=plt.hist((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"LLC4320\")\n",
    "plt.xlim([-.5e-8,3.5e-8])\n",
    "plt.ylim([0,3e5])\n",
    "plt.legend()\n",
    "plt.xlabel('$\\overline{wb}^z$',fontsize=12)\n",
    "\n",
    "plt.subplot(132)\n",
    "_=plt.hist(((0.07*17e3/np.maximum(Lf_BD23_test[:],5000))*WB_FK_test[:]).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"Bodner23 param\")\n",
    "_=plt.hist((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"LLC4320\")\n",
    "plt.xlim([-.5e-8,3.5e-8])\n",
    "plt.ylim([0,3e5])\n",
    "plt.legend()\n",
    "plt.xlabel('$\\overline{wb}^z$',fontsize=12)\n",
    "\n",
    "\n",
    "plt.subplot(133)\n",
    "_=plt.hist((cnn_submeso(b_x.to(device)).detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"CNN prediction\")\n",
    "_=plt.hist((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten(),bins=1000,alpha=0.5,histtype='stepfilled',label=\"LLC4320\")\n",
    "plt.xlim([-.5e-8,3.5e-8])\n",
    "plt.ylim([0,3e5])\n",
    "plt.legend()\n",
    "plt.xlabel('$\\overline{wb}^z$',fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "#plt.savefig(BASE+'plots/cnn_prediction_hist.png')\n",
    "#plt.xlabel('$\\overline{wb}^z$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_da = xr.DataArray((b_y.detach().cpu().numpy()*std_tile + mean_tile), dims=[\"time\", \"axis\", \"i\",\"j\"])[:,0,:,:]\n",
    "param_FK11_da = xr.DataArray((0.07*25e3/np.maximum(Lf_FK11_test[:],5000))*WB_FK_test[:], dims=[\"time\", \"i\",\"j\"])\n",
    "param_BD23_da = xr.DataArray((0.07*25e3/np.maximum(Lf_BD23_test[:],5000))*WB_FK_test[:], dims=[\"time\", \"i\",\"j\"])\n",
    "prediction_da = xr.DataArray((cnn_submeso(b_x.to(device)).detach().cpu().numpy()*std_tile + mean_tile), dims=[\"time\", \"axis\", \"i\",\"j\"])[:,0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xrft\n",
    "\n",
    "# target vs prediction spectra\n",
    "\n",
    "WsBs_target_spectra = xrft.isotropic_power_spectrum(target_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "WsBs_prediction_spectra = xrft.isotropic_power_spectrum(prediction_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "WsBs_param_FK11_spectra = xrft.isotropic_power_spectrum(param_FK11_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n",
    "WsBs_param_BD23_spectra = xrft.isotropic_power_spectrum(param_BD23_da,dim=['i','j'], \n",
    "                                           detrend='linear', window=True).compute().mean('time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(4, 10))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(311)\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, 5e-3*WsBs_target_spectra.freq_r**-2, '--',linewidth=1.5,color='black', label='k^{-2}')\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, 2e-3*WsBs_target_spectra.freq_r**-3, '--',linewidth=1.5,color='gray', label='k^{-3}')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_target_spectra,linewidth=3, label='target')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_prediction_spectra,linewidth=3, label='prediction')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_param_FK11_spectra,linewidth=3, label='FK11')\n",
    "plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_param_BD23_spectra,linewidth=3, label='Bodner23')\n",
    "#plt.loglog(WsBs_target_spectra.freq_r*2*np.pi, WsBs_param_BD_spectra*WsBs_target_spectra.freq_r,linewidth=3, label='BD23')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('$k_r$')\n",
    "#plt.ylabel('Normalized isotropic spectra')\n",
    "plt.title('WsBs')\n",
    "#plt.ylim([1e-3,1e-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH_NN+'test_prediction_7L_k7.npy',cnn_submeso(b_x.to(device)).detach().cpu().numpy()[:,0,:,:])\n",
    "np.save(PATH_NN+'test_target_7L_k7.npy',b_y[:,0,:,:])\n",
    "np.save(PATH_NN+'test_WB_FK.npy',WB_FK_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = np.load(PATH_NN+'test_prediction_7L_k7.npy')\n",
    "test_target = np.load(PATH_NN+'test_target_7L_k7.npy')\n",
    "test_FK = np.load(PATH_NN+'test_WB_FK.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "it =2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.pcolor(test_prediction[it],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb prediction')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.pcolor(test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb target')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.pcolor(test_FK[it],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb FK')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "it =2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 6))\n",
    "fig.tight_layout() # Or equivalently,  \"plt.tight_layout()\"\n",
    "\n",
    "plt.subplot(231)\n",
    "plt.pcolor(test_prediction[it]-test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb prediction')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.pcolor(test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb target')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.pcolor(test_FK[it]-test_target[it,:,:],cmap='coolwarm',vmin=-1.5,vmax =1.5)\n",
    "plt.title('wb FK')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.hist(test_target.flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"target\")\n",
    "_=plt.hist(test_prediction.flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"prediction\")\n",
    "_=plt.hist(WB_FK_test.flatten(),bins=500,alpha=0.5,histtype='stepfilled',label=\"FK08\")\n",
    "\n",
    "\n",
    "plt.ylim([0,.3e6])\n",
    "plt.xlim([-3,3])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_FK11_param = xr.DataArray(((0.07*25e3/np.maximum(Lf_FK11_test[:],5000))*WB_FK_test[:]).flatten())\n",
    "WB_Bodner23_param = xr.DataArray(((0.07*25e3/np.maximum(Lf_BD23_test[:],5000))*WB_FK_test[:]).flatten())\n",
    "WB_sg = xr.DataArray((b_y.detach().cpu().numpy()*std_tile + mean_tile).flatten())\n",
    "WB_cnn = xr.DataArray((cnn_submeso(b_x.to(device)).detach().cpu().numpy()*std_tile + mean_tile).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_Bodner23_param.name = 'Bodner23'\n",
    "WB_FK11_param.name = 'FK11'\n",
    "WB_sg.name = 'WB_subgrid'\n",
    "WB_cnn.name = 'WB_cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhistogram.xarray import histogram as xhist\n",
    "\n",
    "xhist_Bodner23 = xhist(WB_Bodner23_param, WB_sg,\n",
    "                       bins=[np.logspace(-10,-6,30), \n",
    "                             np.concatenate((-np.logspace(-6,-10,30),\n",
    "                                             np.concatenate((np.linspace(-9e-11,9e-11,5),\n",
    "                                                             np.logspace(-10,-6,30)))))\n",
    "                            ]\n",
    "                      ).compute()\n",
    "\n",
    "xhist_FK11 = xhist(WB_FK11_param, \n",
    "      WB_sg,\n",
    "      bins=[np.logspace(-10,-6,30), \n",
    "            np.concatenate((-np.logspace(-6,-10,30),\n",
    "                            np.concatenate((np.linspace(-9e-11,9e-11,5),\n",
    "                                            np.logspace(-10,-6,30)))))\n",
    "           ]\n",
    "     ).compute()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xhist_CNN = xhist(WB_cnn, \n",
    "      WB_sg,\n",
    "      bins=[np.logspace(-10,-6,30), \n",
    "            np.concatenate((-np.logspace(-6,-10,30),\n",
    "                            np.concatenate((np.linspace(-9e-11,9e-11,5),\n",
    "                                            np.logspace(-10,-6,30)))))\n",
    "           ]\n",
    "     ).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "xhist_Bodner23.plot(ax=ax, cmap='Blues')\n",
    "\n",
    "ax.plot(np.linspace(1e-10,1e-6), np.linspace(1e-10,1e-6), c='grey', ls='--')\n",
    "ax.plot(np.linspace(-1e-6,-1e-10), np.linspace(1e-6,1e-10), c='grey', ls='--')\n",
    "ax.set_xscale('symlog', linthresh=1e-10)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-10,1e-6])\n",
    "ax.set_xlim([-1e-6,1e-6])\n",
    "ax.set_xlabel(r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\", fontsize=13)\n",
    "ax.set_ylabel(r\"$\\Psi^{\\tt Bod}\\times\\overline{\\langle b^m}^z$\", \n",
    "              fontsize=13)\n",
    "\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb_medianCr-hist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "xhist_FK11.plot(ax=ax, cmap='Blues')\n",
    "\n",
    "ax.plot(np.linspace(1e-10,1e-6), np.linspace(1e-10,1e-6), c='grey', ls='--')\n",
    "ax.plot(np.linspace(-1e-6,-1e-10), np.linspace(1e-6,1e-10), c='grey', ls='--')\n",
    "ax.set_xscale('symlog', linthresh=1e-10)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-10,1e-6])\n",
    "ax.set_xlim([-1e-6,1e-6])\n",
    "ax.set_xlabel(r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\", fontsize=13)\n",
    "ax.set_ylabel(r\"$\\Psi^{\\tt FK}\\times\\overline{\\nabla b}^z$\", \n",
    "              fontsize=13)\n",
    "\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb_medianCr-hist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "xhist_CNN.plot(ax=ax, vmax=10000, cmap='Blues')\n",
    "\n",
    "ax.plot(np.linspace(1e-10,1e-6), np.linspace(1e-10,1e-6), c='grey', ls='--')\n",
    "ax.plot(np.linspace(-1e-6,-1e-10), np.linspace(1e-6,1e-10), c='grey', ls='--')\n",
    "ax.set_xscale('symlog', linthresh=1e-10)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim([1e-10,1e-6])\n",
    "ax.set_xlim([-1e-6,1e-6])\n",
    "ax.set_xlabel(r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\", fontsize=13)\n",
    "ax.set_ylabel(r\"$CNN prediction$\", \n",
    "              fontsize=13)\n",
    "\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb_medianCr-hist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "#xhist(WB_Bodner23_param, \n",
    "#      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "#                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "#                                            np.logspace(-11,-4,30)))))\n",
    "#           ]\n",
    "     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "xhist(WB_cnn, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"CNN\")\n",
    "xhist(WB_FK11_param, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "xhist(WB_sg, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\")\n",
    "ax.set_xscale('symlog', linthresh=1e-11)\n",
    "ax.set_xlabel(r\"[m$^2$ s$^{-3}$]\", fontsize=13)\n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb-1Dhist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "#xhist(WB_Bodner23_param, \n",
    "#      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "#                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "#                                            np.logspace(-11,-4,30)))))\n",
    "#           ]\n",
    "#     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "#xhist(WB_cnn, \n",
    "#      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "#                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "#                                            np.logspace(-11,-4,30)))))\n",
    "#           ]\n",
    "#     ).plot(ax=ax, label=r\"CNN\")\n",
    "xhist(WB_FK11_param, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"Bodner23\")\n",
    "xhist(WB_sg, \n",
    "      bins=[np.concatenate((-np.logspace(-6,-11,30),\n",
    "                            np.concatenate((np.linspace(-9e-12,9e-12,5),\n",
    "                                            np.logspace(-11,-4,30)))))\n",
    "           ]\n",
    "     ).plot(ax=ax, label=r\"$\\overline{wb}^z-\\overline{w}^z\\overline{b}^z$\")\n",
    "ax.set_xscale('symlog', linthresh=1e-11)\n",
    "ax.set_xlabel(r\"[m$^2$ s$^{-3}$]\", fontsize=13)\n",
    "ax.legend(loc='upper left', fontsize=12)\n",
    "#plt.savefig(op.join(sdir,'Figs/eNATL60-wb-1Dhist_region%02d.pdf' % reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
