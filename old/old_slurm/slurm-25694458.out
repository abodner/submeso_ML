WARNING: Could not find any nv files on this host!
/ext3/miniconda3/lib/python3.9/site-packages/numba/core/cpu.py:97: UserWarning: Numba extension module 'sparse._numba_extension' failed to load due to 'ContextualVersionConflict((numpy 1.22.4 (/ext3/miniconda3/lib/python3.9/site-packages), Requirement.parse('numpy<1.22,>=1.18'), {'numba'}))'.
  numba.core.entrypoints.init_all()
PREPROCESS CASE lat_-57_-42_lon_23_37/
2022-10-06 06:33:53.215430
T,S,U,V,W FROM LLC4320
2022-10-06 06:33:53.215499
/ext3/miniconda3/lib/python3.9/site-packages/xarray/core/indexing.py:1380: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  value = value[(slice(None),) * axis + (subkey,)]
/ext3/miniconda3/lib/python3.9/site-packages/xarray/core/indexing.py:1380: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  value = value[(slice(None),) * axis + (subkey,)]
/ext3/miniconda3/lib/python3.9/site-packages/xarray/core/indexing.py:1380: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  value = value[(slice(None),) * axis + (subkey,)]
/ext3/miniconda3/lib/python3.9/site-packages/xarray/core/indexing.py:1380: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  value = value[(slice(None),) * axis + (subkey,)]
/ext3/miniconda3/lib/python3.9/site-packages/xarray/core/indexing.py:1380: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  value = value[(slice(None),) * axis + (subkey,)]
CALCULATE B
2022-10-06 09:51:17.683300
SAVE SNAPSHOT: B,U,V,W
2022-10-06 09:58:35.047325
SAVE SNAPSHOT: MLD
2022-10-06 09:58:57.353175
FILTER
2022-10-06 09:58:58.168959
COARSE-GRAIN
2022-10-06 11:36:22.766224
/ext3/miniconda3/lib/python3.9/site-packages/dask/array/core.py:450: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  o = func(*args, **kwargs)
MLD AVERAGE
2022-10-06 11:36:52.913686
SAVE NETCDF: MLD AVG LOWRES
meso fields
Traceback (most recent call last):
  File "/home/ab10313/submeso_ML/preprocess_code/preprocess_llc4320_save_lat_-57_-42_lon_23_37.py", line 383, in <module>
    Um_lowres_MLD.to_netcdf(PATH+'data/Um_lowres_MLD.nc')
  File "/ext3/miniconda3/lib/python3.9/site-packages/xarray/core/dataarray.py", line 3217, in to_netcdf
    return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
  File "/ext3/miniconda3/lib/python3.9/site-packages/xarray/backends/api.py", line 1210, in to_netcdf
    dump_to_store(
  File "/ext3/miniconda3/lib/python3.9/site-packages/xarray/backends/api.py", line 1257, in dump_to_store
    store.store(variables, attrs, check_encoding, writer, unlimited_dims=unlimited_dims)
  File "/ext3/miniconda3/lib/python3.9/site-packages/xarray/backends/common.py", line 267, in store
    self.set_variables(
  File "/ext3/miniconda3/lib/python3.9/site-packages/xarray/backends/common.py", line 305, in set_variables
    target, source = self.prepare_variable(
  File "/ext3/miniconda3/lib/python3.9/site-packages/xarray/backends/scipy_.py", line 226, in prepare_variable
    self.ds.createVariable(name, data.dtype, variable.dims)
  File "/home/ab10313/.local/lib/python3.9/site-packages/scipy/io/netcdf.py", line 388, in createVariable
    data = empty(shape_, dtype=type.newbyteorder("B"))  # convert to big endian always for NetCDF 3
numpy.core._exceptions._ArrayMemoryError: Unable to allocate 2.10 TiB for an array with shape (430, 41, 41, 1186, 672) and data type >f4
