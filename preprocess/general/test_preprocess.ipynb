{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xarray as xr\n",
    "import xgcm \n",
    "from fastjmd95 import jmd95numba \n",
    "import glob\n",
    "\n",
    "\n",
    "#path\n",
    "BASE = '/scratch/ab10313/pleiades/'\n",
    "\n",
    "PATH_LIST_2d = glob.glob(BASE+'*_smooth/2d_data/')\n",
    "PATH_LIST_3d = glob.glob(BASE+'*_smooth/3d_data/')\n",
    "PATH_LIST_PP = glob.glob(BASE+'*_smooth/preprcossed_data/')\n",
    "PATH_LIST_PP_surface = glob.glob(BASE+'*_smooth/preprcossed_data_surface/')\n",
    "PATH_LIST_PP_interior = glob.glob(BASE+'*_smooth/preprcossed_data_interior/')\n",
    "\n",
    "for i_file in np.arange(0,len(PATH_LIST_PP)):\n",
    "   \n",
    "    # paths to dataset\n",
    "    PATH_2d = PATH_LIST_2d[i_file]\n",
    "    PATH_3d = PATH_LIST_3d[i_file]\n",
    "\n",
    "    # make diirectory for preprocessed variables\n",
    "    PATH_PP = PATH_LIST_PP[i_file]\n",
    "\n",
    "    PATH_PP_S = PATH_PP+'surface/'\n",
    "    isExist = os.path.exists(PATH_PP_S)\n",
    "    if not isExist:\n",
    "        os.mkdir(PATH_PP_S)\n",
    "\n",
    "    PATH_PP_I = PATH_PP+'interior/'\n",
    "    isExist = os.path.exists(PATH_PP_I)\n",
    "    if not isExist:\n",
    "        os.mkdir(PATH_PP_I)\n",
    "\n",
    "\n",
    "\n",
    "    # load 2d data\n",
    "    ds_HBL = xr.open_dataset(PATH_2d+'ds_HBL.nc',engine=\"h5netcdf\")\n",
    "    ds_Q = xr.open_dataset(PATH_2d+'ds_Q.nc',engine=\"h5netcdf\")\n",
    "    ds_TAUX = xr.open_dataset(PATH_2d+'ds_TAUX.nc',engine=\"h5netcdf\")\n",
    "    ds_TAUY = xr.open_dataset(PATH_2d+'ds_TAUY.nc',engine=\"h5netcdf\")\n",
    "\n",
    "\n",
    "    # load 3d data\n",
    "    ds_T = xr.open_dataset(PATH_3d+'ds_T.nc',engine=\"h5netcdf\")\n",
    "    ds_S = xr.open_dataset(PATH_3d+'ds_S.nc',engine=\"h5netcdf\")\n",
    "    ds_U = xr.open_dataset(PATH_3d+'ds_U.nc',engine=\"h5netcdf\")\n",
    "    ds_V = xr.open_dataset(PATH_3d+'ds_V.nc',engine=\"h5netcdf\")\n",
    "    ds_W = xr.open_dataset(PATH_3d+'ds_W.nc',engine=\"h5netcdf\")\n",
    "\n",
    "\n",
    "\n",
    "    # find min and max i and j to crop to 10X10 degrees\n",
    "\n",
    "    i_min = np.max([ds_HBL.i.min().values,ds_Q.i.min().values, ds_TAUX.i_g.min().values, ds_TAUY.i.min().values,\n",
    "                    ds_T.i.min().values, ds_S.i.min().values, ds_U.i_g.min().values, ds_V.i.min().values, ds_W.i.min().values])\n",
    "\n",
    "\n",
    "    i_max = np.min([ds_HBL.i.max().values,ds_Q.i.max().values, ds_TAUX.i_g.max().values, ds_TAUY.i.max().values,\n",
    "                    ds_T.i.max().values, ds_S.i.max().values, ds_U.i_g.max().values, ds_V.i.max().values, ds_W.i.max().values])\n",
    "\n",
    "\n",
    "    j_min = np.max([ds_HBL.j.min().values,ds_Q.j.min().values, ds_TAUX.j.min().values, ds_TAUY.j_g.min().values,\n",
    "                    ds_T.j.min().values, ds_S.j.min().values, ds_U.j.min().values, ds_V.j_g.min().values, ds_W.j.min().values])\n",
    "\n",
    "\n",
    "    j_max = np.min([ds_HBL.j.max().values,ds_Q.j.max().values, ds_TAUX.j.max().values, ds_TAUY.j_g.max().values,\n",
    "                    ds_T.j.max().values, ds_S.j.max().values, ds_U.j.max().values, ds_V.j_g.max().values, ds_W.j.max().values])\n",
    "\n",
    "\n",
    "\n",
    "    #define slice to 480 index\n",
    "\n",
    "    if i_min+480>i_max:\n",
    "        print('cropped region error in i')\n",
    "    elif j_min+480>j_max:\n",
    "        print('cropped region error in j')\n",
    "    else:\n",
    "        i_slice = slice(i_min,i_min+480)\n",
    "        j_slice = slice(j_min,j_min+480)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # merge datasets\n",
    "    ds_2d =xr.merge([ds_HBL.sel(i=i_slice,j=j_slice), ds_Q.sel(i=i_slice,j=j_slice),\n",
    "                     ds_TAUX.sel(i_g=i_slice,j=j_slice), ds_TAUY.sel(i=i_slice,j_g=j_slice)])\n",
    "\n",
    "\n",
    "    ds_3d =xr.merge([ds_T.sel(i=i_slice,j=j_slice), ds_S.sel(i=i_slice,j=j_slice),\n",
    "                     ds_U.sel(i_g=i_slice,j=j_slice), ds_V.sel(i=i_slice,j_g=j_slice), ds_W.sel(i=i_slice,j=j_slice)])\n",
    "\n",
    "\n",
    "\n",
    "    # define grids \n",
    "\n",
    "    grid_2d = xgcm.Grid(ds_2d)\n",
    "\n",
    "    grid_3d = xgcm.Grid(ds_3d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # sigma from temp and salt, using the fastjmd95 package\n",
    "\n",
    "\n",
    "    # reference density \n",
    "    rho0 = 1000 #kg/m^3\n",
    "\n",
    "    # potential density anomaly \n",
    "    # with the reference pressure of 0 dbar and ρ0 = 1000 kg m−3\n",
    "    sigma0 = jmd95numba.rho(ds_3d.Salt.chunk(chunks={'time': 1, 'j': ds_3d.j.size, 'i': ds_3d.i.size}),\n",
    "                             ds_3d.Theta.chunk(chunks={'time': 1, 'j': ds_3d.j.size, 'i': ds_3d.i.size}), 0) - rho0\n",
    "\n",
    "    sigma0 = sigma0.rename('sigma0')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # sigma0 at 10m depth for reference\n",
    "\n",
    "    sigma0_10m = sigma0.isel(k=6).broadcast_like(sigma0).chunk(chunks={'time': 1, 'j': ds_3d.j.size, 'i': ds_3d.i.size})\n",
    "    delta_sigma = sigma0 - sigma0_10m\n",
    "    del sigma0_10m\n",
    "\n",
    "\n",
    "\n",
    "    # gravity\n",
    "    G = 9.81 #m/s^2\n",
    "\n",
    "    # buoyancy\n",
    "    B = -G*sigma0/rho0\n",
    "    B = B.rename('Buoyancy')\n",
    "\n",
    "    # save surface buoyancy buoyancy:\n",
    "    B.isel(k=0).to_netcdf(PATH_PP_S+'B_surf.nc',engine='h5netcdf')\n",
    "    \n",
    "    # save buoyancy averaged over mixed layer depth:\n",
    "    B.where(delta_sigma<=0.03).mean(dim=\"k\",skipna=True).to_netcdf(PATH_PP_I+'B.nc',engine='h5netcdf')\n",
    "\n",
    "\n",
    "    # vertical buoyancy gradient (stratification) note the minus sign because z is negative\n",
    "    Nsquared = -B.diff(dim='k')/B.drF\n",
    "    Nsquared.where(delta_sigma<=0.03).mean(dim=\"k\",skipna=True).to_netcdf(PATH_PP_I+'Nsquared.nc',engine='h5netcdf')\n",
    "    del Nsquared\n",
    "\n",
    "\n",
    "\n",
    "    # mixed layer depth\n",
    "    HML = sigma0.Z.broadcast_like(sigma0).where(delta_sigma<=0.03).min(dim=\"k\",skipna=True).chunk(chunks={'time': 1, 'j': sigma0.j.size, 'i': sigma0.i.size}).rename('Mixed Layer Depth')\n",
    "    HML.to_netcdf(PATH_PP_I+'HML.nc',engine='h5netcdf')\n",
    "    del HML, sigma0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # interp velocities and buoyancy fluxes, average over MLD\n",
    "\n",
    "    W_interp = grid_3d.interp(ds_3d.W,'Z', boundary='extend')\n",
    "    W_interp.where(delta_sigma<=0.03).mean(dim=\"k\",skipna=True).to_netcdf(PATH_PP_I+'W.nc',engine='h5netcdf')\n",
    "\n",
    "    WB = W_interp * B\n",
    "    WB.where(delta_sigma<=0.03).mean(dim=\"k\",skipna=True).to_netcdf(PATH_PP_I+'WB.nc',engine='h5netcdf')\n",
    "\n",
    "    del W_interp, WB\n",
    "\n",
    "    # lat lon\n",
    "    lat = ds_2d.YC.mean('i')\n",
    "    lon = ds_2d.XC.mean('j')\n",
    "\n",
    "    lat.to_netcdf(PATH_PP+'lat.nc',engine='h5netcdf')\n",
    "    lon.to_netcdf(PATH_PP+'lon.nc',engine='h5netcdf')\n",
    "\n",
    "    # Coriolis \n",
    "    omega = 7.2921e-5\n",
    "\n",
    "    FCOR = xr.zeros_like(B.isel(time=0,k=0))\n",
    "\n",
    "    for ii in range(len(lat)):\n",
    "            FCOR[ii,:] = 2*omega*np.sin(lat[ii]* np.pi / 180.)\n",
    "\n",
    "    FCOR.to_netcdf(PATH_PP+'FCOR.nc',engine='h5netcdf')\n",
    "\n",
    "    del FCOR, lat, lon, B\n",
    "\n",
    "\n",
    "    \n",
    "    # interp tau\n",
    "\n",
    "    TAUX_interp = grid_2d.interp(ds_2d.oceTAUX,'X', boundary='extend')\n",
    "    TAUX_interp.to_netcdf(PATH_PP_S+'TAUX.nc',engine='h5netcdf')\n",
    "\n",
    "    TAUY_interp = grid_2d.interp(ds_2d.oceTAUY,'Y', boundary='extend')\n",
    "    TAUY_interp.to_netcdf(PATH_PP_S+'TAUY.nc',engine='h5netcdf')\n",
    "\n",
    "\n",
    "    # save HBL\n",
    "\n",
    "    ds_2d.KPPhbl.to_netcdf(PATH_PP_I+'HBL.nc',engine='h5netcdf')\n",
    "\n",
    "\n",
    "    # save Q\n",
    "    ds_2d.oceQnet.to_netcdf(PATH_PP_S+'Q.nc',engine='h5netcdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "submeso_env",
   "language": "python",
   "name": "submeso_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
