{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate NN data - surface and interior variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "from xgcm import Grid\n",
    "\n",
    "\n",
    "#path\n",
    "BASE = '/scratch/ab10313/pleiades/'\n",
    "\n",
    "#PATH_NN = BASE+'NN_data_smooth/'\n",
    "#os.mkdir(PATH_NN)\n",
    "\n",
    "# NN data path: surface\n",
    "PATH_NN_surface = BASE+'NN_data_surface/'\n",
    "#os.mkdir(PATH_NN_surface)\n",
    "\n",
    "# NN data path: inter\n",
    "PATH_NN_interior = BASE+'NN_data_interior/'\n",
    "#os.mkdir(PATH_NN_interior)\n",
    "\n",
    "\n",
    "PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/')\n",
    "# remove two regions without strong w'b'\n",
    "#PATH_LIST_full.remove(BASE+'04_equator_atlantic_smooth/preprcossed_data/')\n",
    "#PATH_LIST_full.remove(BASE+'10_north_pacific_smooth/preprcossed_data/')\n",
    "#PATH_LIST_full.remove(BASE+'15_bengal_smooth/preprcossed_data/') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# course grain\n",
    "\n",
    "def coarse_grain(data,time_factor, grid_factor):\n",
    "    if len(data.dims) == 3:\n",
    "        data_cg = data.rolling(time=time_factor, center=True).mean().dropna(dim=\"time\", how=\"all\").coarsen(i=grid_factor,j=grid_factor, boundary=\"trim\").mean()\n",
    "    elif len(data.dims) == 2:\n",
    "        data_cg = np.tile(data.coarsen(i=grid_factor,j=grid_factor, boundary=\"trim\").mean(),(846,1,1))\n",
    "    return data_cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "def normalize(data):\n",
    "    normalized_data =  (data - np.nanmean(data))/np.nanstd(data)\n",
    "    data_mean = np.tile(np.nanmean(data),(846))\n",
    "    data_std = np.tile(np.nanstd(data),(846))\n",
    "    return normalized_data,data_mean, data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits datasets along the spacial axes and concats them back into single array under time\n",
    "\n",
    "def load_data_norm(var_name_string,time_factor=14,grid_factor=12):\n",
    "    PATH_LIST = glob.glob(BASE+'*_smooth/preprcossed_data/'+var_name_string+'.nc') \n",
    "    data_0 = xr.open_dataarray(PATH_LIST[0])\n",
    "    data_smooth_0 = coarse_grain(data_0,time_factor,grid_factor)\n",
    "    data_smooth_norm_0, data_mean_0, data_std_0 = normalize(data_smooth_0)\n",
    "    data_app = data_smooth_norm_0\n",
    "    data_mean_app = data_mean_0\n",
    "    data_std_app = data_std_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST)):\n",
    "        PATH = PATH_LIST[i_file]\n",
    "        data = xr.open_dataarray(PATH)\n",
    "        data_smooth = coarse_grain(data,time_factor,grid_factor)\n",
    "        data_smooth_norm, data_mean, data_std = normalize(data_smooth)\n",
    "        data_app = np.concatenate((data_app,data_smooth_norm),axis=0)\n",
    "        data_mean_app = np.concatenate((data_mean_app,data_mean),axis=0)\n",
    "        data_std_app = np.concatenate((data_std_app,data_std),axis=0)\n",
    "    return data_app, data_mean_app, data_std_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(var_name_string,time_factor=14,grid_factor=12):\n",
    "    PATH_LIST = glob.glob(BASE+'*_smooth/preprcossed_data/interior/'+var_name_string+'.nc') \n",
    "    data_0 = xr.open_dataarray(PATH_LIST[0])\n",
    "    data_smooth_0 = coarse_grain(data_0,time_window,coarsen_factor)\n",
    "    data_app = data_smooth_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST)):\n",
    "        PATH = PATH_LIST[i_file]\n",
    "        data = xr.open_dataarray(PATH)\n",
    "        data_smooth = coarse_grain(data,time_window,coarsen_factor)\n",
    "        data_app = np.concatenate((data_app,data_smooth),axis=0)\n",
    "    return data_app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WB_sg_target(PATH,time_factor=14,grid_factor=12):\n",
    "    # WB\n",
    "    B = coarse_grain(xr.open_dataarray(PATH+'B.nc'),time_factor,grid_factor).values\n",
    "    W = coarse_grain(xr.open_dataarray(PATH+'W.nc'),time_factor,grid_factor).values\n",
    "    WB = coarse_grain(xr.open_dataarray(PATH+'WB.nc'),time_factor,grid_factor).values\n",
    "    \n",
    "    # WB subgrid\n",
    "    WB_sg = WB - W*B\n",
    "    WB_sg_norm, WB_sg_mean, WB_sg_std = normalize(WB_sg)\n",
    "    return WB_sg_norm, WB_sg_mean, WB_sg_std\n",
    "\n",
    "\n",
    "    \n",
    "def load_data_WB(time_factor=14,grid_factor=12):\n",
    "    PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/interior/') \n",
    "    WB_sg_norm_0, WB_sg_mean_0, WB_sg_std_0 = WB_sg_target(PATH_LIST_full[0])\n",
    "    data_app = WB_sg_norm_0\n",
    "    data_mean_app = WB_sg_mean_0\n",
    "    data_std_app = WB_sg_std_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST_full)):\n",
    "        WB_sg_norm, WB_sg_mean, WB_sg_std = WB_sg_target(PATH_LIST_full[i_file])\n",
    "        data_app = np.concatenate((data_app,WB_sg_norm),axis=0)\n",
    "        data_mean_app = np.concatenate((data_mean_app,WB_sg_mean),axis=0)\n",
    "        data_std_app = np.concatenate((data_std_app,WB_sg_std),axis=0)\n",
    "    return data_app, data_mean_app, data_std_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_B_mag(PATH,time_factor=14,grid_factor=12):\n",
    "    # grad_B\n",
    "    B = coarse_grain(xr.open_dataarray(PATH+'B_surf.nc'),time_factor=14,grid_factor=12)\n",
    "    B_x = (B.diff(dim='i')/(12*B.dxF)).interp(i=B.i,j=B.j,kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    B_y = (B.diff(dim='j')/(12*B.dyF)).interp(i=B.i,j=B.j,kwargs={\"fill_value\": \"extrapolate\"})\n",
    "    grad_B = np.sqrt(B_y**2 + B_x**2).values\n",
    "\n",
    "    grad_B_norm, grad_B_mean, grad_B_std = normalize(grad_B)\n",
    "    return grad_B_norm, grad_B_mean, grad_B_std\n",
    "\n",
    "\n",
    "    \n",
    "def load_data_grad_B(time_factor=14,grid_factor=12):\n",
    "    PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/surface/') \n",
    "    grad_B_norm_0, grad_B_mean_0, grad_B_std_0 = grad_B_mag(PATH_LIST_full[0])\n",
    "    data_app = grad_B_norm_0\n",
    "    data_mean_app = grad_B_mean_0\n",
    "    data_std_app = grad_B_std_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST_full)):\n",
    "        grad_B_norm, grad_B_mean, grad_B_std = grad_B_mag(PATH_LIST_full[i_file])\n",
    "        data_app = np.concatenate((data_app,grad_B_norm),axis=0)\n",
    "        data_mean_app = np.concatenate((data_mean_app,grad_B_mean),axis=0)\n",
    "        data_std_app = np.concatenate((data_std_app,grad_B_std),axis=0)\n",
    "    return data_app, data_mean_app, data_std_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TAU_mag(PATH,time_factor=14,grid_factor=12):\n",
    "    # grad_B\n",
    "    # wind stress\n",
    "    TAUX = coarse_grain(xr.open_dataarray(PATH+'TAUX.nc'),time_factor=14,grid_factor=12)\n",
    "    TAUY = coarse_grain(xr.open_dataarray(PATH+'TAUY.nc'),time_factor=14,grid_factor=12)\n",
    "    TAU = np.sqrt(TAUY**2 + TAUX**2).values\n",
    "\n",
    "    TAU_norm, TAU_mean, TAU_std = normalize(TAU)\n",
    "    return TAU_norm, TAU_mean, TAU_std\n",
    "\n",
    "\n",
    "    \n",
    "def load_data_TAU_mag(time_factor=14,grid_factor=12):\n",
    "    PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/surface/') \n",
    "    TAU_norm_0, TAU_mean_0, TAU_std_0 = TAU_mag(PATH_LIST_full[0])\n",
    "    data_app = TAU_norm_0\n",
    "    data_mean_app = TAU_mean_0\n",
    "    data_std_app = TAU_std_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST_full)):\n",
    "        TAU_norm, TAU_mean, TAU_std = TAU_mag(PATH_LIST_full[i_file])\n",
    "        data_app = np.concatenate((data_app,TAU_norm),axis=0)\n",
    "        data_mean_app = np.concatenate((data_mean_app,TAU_mean),axis=0)\n",
    "        data_std_app = np.concatenate((data_std_app,TAU_std),axis=0)\n",
    "    return data_app, data_mean_app, data_std_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coarse-res horizontal buoyancy gradient \n",
    "grad_B_norm, grad_B_mean, grad_B_std = load_data_grad_B(time_factor=14,grid_factor=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WB\n",
    "WB_sg_norm, WB_sg_mean, WB_sg_std = load_data_WB(time_factor=14,grid_factor=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coriolis\n",
    "FCOR, FCOR_mean, FCOR_std = load_data_norm('FCOR',time_factor=14,grid_factor=12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H mixed layer (NEGATIVE)\n",
    "HML, HML_mean, HML_std = load_data_norm('HML',time_factor=14,grid_factor=12) \n",
    "\n",
    "#HML = coarse_grain(xr.open_dataarray(PATH_gulf+'HML.nc'),time_factor=14,grid_factor=12)\n",
    "#HML = -HML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nsquared \n",
    "#also making Nsquared very small but not zero to avoid singularity\n",
    "# look at gsw to compute a different way. \n",
    "# IMPORTANT: SETTING TO 1e-5 just to move forward for now\n",
    "Nsquared, Nsquared_mean, Nsquared_std = load_data_norm('Nsquared')\n",
    "\n",
    "#Nsquared = coarse_grain(xr.open_dataarray(PATH_gulf+'Nsquared.nc'),time_factor=14,grid_factor=12)\n",
    "#Nsquared = Nsquared.where(Nsquared>=0).where(Nsquared<=0).fillna(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind stress\n",
    "TAU_norm, TAU_mean, TAU_std  = load_data_TAU_mag(time_factor=14,grid_factor=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface heat flux\n",
    "Q, Q_mean, Q_std = load_data_norm('Q',time_factor=14,grid_factor=12)\n",
    "#Q = coarse_grain(xr.open_dataarray(PATH_gulf+'Q.nc'),time_factor=14,grid_factor=12)\n",
    "#Q = Q.where(Q<0.).fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H boundary layer (POSITIVE)\n",
    "HBL, HBL_mean, HBL_std = load_data_norm('HBL',time_factor=14,grid_factor=12) \n",
    "\n",
    "#HBL = coarse_grain(xr.open_dataarray(PATH_gulf+'HBL.nc'),time_factor=14,grid_factor=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell size for weighted average\n",
    "PATH_LIST = glob.glob(BASE+'*_smooth/preprcossed_data/interior/B.nc') \n",
    "B_0 = xr.open_dataarray(PATH_LIST[0])\n",
    "data_0 = (B_0.dxF**2 + B_0.dyF**2)**0.5\n",
    "data_smooth_0 = coarse_grain(data_0,time_factor=14,grid_factor=12)\n",
    "data_app = data_smooth_0\n",
    "for i_file in np.arange(1,len(PATH_LIST)):\n",
    "    PATH = PATH_LIST[i_file]\n",
    "    B_i = xr.open_dataarray(PATH)\n",
    "    data = (B_i.dxF**2 + B_i.dyF**2)**0.5\n",
    "    data_smooth = coarse_grain(data,time_factor=14,grid_factor=12)\n",
    "    data_app = np.concatenate((data_app,data_smooth),axis=0)\n",
    "\n",
    "drF = data_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save normalized NN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized NN data\n",
    "\n",
    "#surface\n",
    "np.save(PATH_NN_surface+'grad_B.npy',grad_B_norm)\n",
    "np.save(PATH_NN_surface+'FCOR.npy',FCOR)\n",
    "np.save(PATH_NN_surface+'TAU.npy',TAU_norm)\n",
    "np.save(PATH_NN_surface+'Q.npy',Q)\n",
    "\n",
    "\n",
    "\n",
    "# interior\n",
    "np.save(PATH_NN_interior+'HML.npy',HML)\n",
    "np.save(PATH_NN_interior+'HBL.npy',HBL)\n",
    "\n",
    "np.save(PATH_NN_interior+'WB_sg.npy',WB_sg_norm)\n",
    "np.save(PATH_NN_interior+'WB_sg_mean.npy',WB_sg_mean)\n",
    "np.save(PATH_NN_interior+'WB_sg_std.npy',WB_sg_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_b, TAU, Q mean and std for going back to physical space\n",
    "\n",
    "np.save(PATH_NN_surface+'grad_B_mean.npy',grad_B_mean)\n",
    "np.save(PATH_NN_surface+'grad_B_std.npy',grad_B_std)\n",
    "\n",
    "np.save(PATH_NN_surface+'TAU_mean.npy',TAU_mean)\n",
    "np.save(PATH_NN_surface+'TAU_std.npy',TAU_std)\n",
    "\n",
    "np.save(PATH_NN_surface+'Q_mean.npy',Q_mean)\n",
    "np.save(PATH_NN_surface+'Q_std.npy',Q_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid cell size for weighted average\n",
    "np.save(PATH_NN_surface+'drF.npy',drF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "submeso_env",
   "language": "python",
   "name": "submeso_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
