{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "\n",
    "#path\n",
    "BASE = '/scratch/ab10313/pleiades/'\n",
    "\n",
    "PATH_NN = BASE+'NN_data_smooth/'\n",
    "#os.mkdir(PATH_NN)\n",
    "\n",
    "PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/')\n",
    "# remove two regions without strong w'b'\n",
    "PATH_LIST_full.remove(BASE+'04_equator_atlantic_smooth/preprcossed_data/')\n",
    "PATH_LIST_full.remove(BASE+'10_north_pacific_smooth/preprcossed_data/')\n",
    "PATH_LIST_full.remove(BASE+'15_bengal_smooth/preprcossed_data/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/ab10313/pleiades/12_agulhas_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/11_new_zealand_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/01_gulf_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/03_south_atlantic_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/05_argentina_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/13_australia_smooth/preprcossed_data/',\n",
       " '/scratch/ab10313/pleiades/14_indian_ocean_smooth/preprcossed_data/']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LIST_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that splits datasets along the spacial axes and concats them back into single array under time\n",
    "\n",
    "def normalize(data):\n",
    "    normalized_data =  (data - np.nanmean(data))/np.nanstd(data)\n",
    "    data_mean = np.tile(np.nanmean(data),(845))\n",
    "    data_std = np.tile(np.nanstd(data),(845))\n",
    "    return normalized_data,data_mean, data_std\n",
    "\n",
    "def smooth(data,time_window, coarsen_factor):\n",
    "    if len(data.dims) == 3:\n",
    "        smoothed_data = data.rolling(time=time_window, center=True).mean().dropna(dim=\"time\", how=\"all\").coarsen(i=coarsen_factor,j=coarsen_factor, boundary=\"trim\").mean()\n",
    "    elif len(data.dims) == 2:\n",
    "        smoothed_data = np.tile(data.coarsen(i=coarsen_factor,j=coarsen_factor, boundary=\"trim\").mean(),(845,1,1))\n",
    "    return smoothed_data\n",
    "\n",
    "\n",
    "def load_data_norm(var_name_string,time_window=15,coarsen_factor=12):\n",
    "    PATH_LIST = glob.glob(BASE+'*_smooth/preprcossed_data/'+var_name_string+'.nc') \n",
    "    data_0 = xr.open_dataarray(PATH_LIST[0])\n",
    "    data_smooth_0 = smooth(data_0,time_window,coarsen_factor)\n",
    "    data_smooth_norm_0, data_mean_0, data_std_0 = normalize(data_smooth_0)\n",
    "    data_app = data_smooth_norm_0\n",
    "    data_mean_app = data_mean_0\n",
    "    data_std_app = data_std_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST)):\n",
    "        PATH = PATH_LIST[i_file]\n",
    "        data = xr.open_dataarray(PATH)\n",
    "        data_smooth = smooth(data,time_window,coarsen_factor)\n",
    "        data_smooth_norm, data_mean, data_std = normalize(data_smooth)\n",
    "        data_app = np.concatenate((data_app,data_smooth_norm),axis=0)\n",
    "        data_mean_app = np.concatenate((data_mean_app,data_mean),axis=0)\n",
    "        data_std_app = np.concatenate((data_std_app,data_std),axis=0)\n",
    "    return data_app, data_mean_app, data_std_app\n",
    "\n",
    "\n",
    "def load_data(var_name_string,time_window=15,coarsen_factor=12):\n",
    "    PATH_LIST = glob.glob(BASE+'*_smooth/preprcossed_data/'+var_name_string+'.nc') \n",
    "    data_0 = xr.open_dataarray(PATH_LIST[0])\n",
    "    data_smooth_0 = smooth(data_0,time_window,coarsen_factor)\n",
    "    data_app = data_smooth_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST)):\n",
    "        PATH = PATH_LIST[i_file]\n",
    "        data = xr.open_dataarray(PATH)\n",
    "        data_smooth = smooth(data,time_window,coarsen_factor)\n",
    "        data_app = np.concatenate((data_app,data_smooth),axis=0)\n",
    "    return data_app\n",
    "\n",
    "def FK_param(PATH,time_window=15,coarsen_factor=12):\n",
    "    # FK08 parameterization inputs\n",
    "    tau = 86400\n",
    "    FCOR = smooth(xr.open_dataarray(PATH+'FCOR.nc'),time_window,coarsen_factor)\n",
    "    B_x = smooth(xr.open_dataarray(PATH+'B_x.nc'),time_window,coarsen_factor).values\n",
    "    B_y = smooth(xr.open_dataarray(PATH+'B_y.nc'),time_window,coarsen_factor).values\n",
    "    grad_B = np.sqrt(B_y**2 + B_x**2)\n",
    "    \n",
    "    HML = smooth(xr.open_dataarray(PATH+'HML.nc'),time_window,coarsen_factor).values\n",
    "    \n",
    "    WB_FK = ((HML**2) * (grad_B**2))/np.sqrt(FCOR**2 + tau**-2)\n",
    "    \n",
    "    return WB_FK\n",
    "    \n",
    "    \n",
    "def load_data_FK(time_window=15,coarsen_factor=12):\n",
    "    PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/') \n",
    "    WB_FK_0 = FK_param(PATH_LIST_full[0])\n",
    "    data_app = WB_FK_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST_full)):\n",
    "        WB_FK = FK_param(PATH_LIST_full[i_file])\n",
    "        data_app = np.concatenate((data_app,WB_FK),axis=0)\n",
    "    return data_app\n",
    "\n",
    "    \n",
    "    \n",
    "def WB_sg_target(PATH,time_window=15,coarsen_factor=12):\n",
    "    # WB\n",
    "    B = smooth(xr.open_dataarray(PATH+'B.nc'),time_window,coarsen_factor).values\n",
    "    W = smooth(xr.open_dataarray(PATH+'W.nc'),time_window,coarsen_factor).values\n",
    "    WB = smooth(xr.open_dataarray(PATH+'WB.nc'),time_window,coarsen_factor).values\n",
    "    \n",
    "    # WB subgrid\n",
    "    WB_sg = WB - W*B\n",
    "    WB_sg_norm, WB_sg_mean, WB_sg_std = normalize(WB_sg)\n",
    "    return WB_sg_norm, WB_sg_mean, WB_sg_std\n",
    "\n",
    "\n",
    "    \n",
    "def load_data_WB(time_window=15,coarsen_factor=12):\n",
    "    PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/') \n",
    "    WB_sg_norm_0, WB_sg_mean_0, WB_sg_std_0 = WB_sg_target(PATH_LIST_full[0])\n",
    "    data_app = WB_sg_norm_0\n",
    "    data_mean_app = WB_sg_mean_0\n",
    "    data_std_app = WB_sg_std_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST_full)):\n",
    "        WB_sg_norm, WB_sg_mean, WB_sg_std = WB_sg_target(PATH_LIST_full[i_file])\n",
    "        data_app = np.concatenate((data_app,WB_sg_norm),axis=0)\n",
    "        data_mean_app = np.concatenate((data_mean_app,WB_sg_mean),axis=0)\n",
    "        data_std_app = np.concatenate((data_std_app,WB_sg_std),axis=0)\n",
    "    return data_app, data_mean_app, data_std_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FK_param_Lf(PATH,time_window=15,coarsen_factor=12):\n",
    "    # FK08 parameterization inputs\n",
    "    tau = 86400\n",
    "    FCOR = smooth(xr.open_dataarray(PATH+'FCOR.nc'),time_window,coarsen_factor)\n",
    "    B_x = smooth(xr.open_dataarray(PATH+'B_x.nc'),time_window,coarsen_factor).values\n",
    "    B_y = smooth(xr.open_dataarray(PATH+'B_y.nc'),time_window,coarsen_factor).values\n",
    "    grad_B = np.sqrt(B_y**2 + B_x**2)\n",
    "    \n",
    "    HML = smooth(xr.open_dataarray(PATH+'HML.nc'),time_window,coarsen_factor).values\n",
    "    #note the minus sign here because z is negative, change this after new preprocessing\n",
    "    #also making Nsquared very small but not zero to avoid singularity\n",
    "    Nsquared = smooth(xr.open_dataarray(PATH+'Nsquared.nc'),time_window,coarsen_factor).values\n",
    "    Nsquared[Nsquared<=0]=1e-12\n",
    "    \n",
    "    # WB FK11 with Lf rescaling factor\n",
    "    ds = 25e3 # approx 1/4 degree resolution  ~ 25km\n",
    "    Lf = (np.sqrt(Nsquared)*HML)/np.sqrt(FCOR**2 + tau**-2)\n",
    "\n",
    "    WB_FK = (ds /Lf)*((HML**2) * (grad_B**2))/np.sqrt(FCOR**2 + tau**-2)\n",
    "    return WB_FK\n",
    "    \n",
    "    \n",
    "def load_data_FK_Lf(time_window=15,coarsen_factor=12):\n",
    "    PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/') \n",
    "    WB_FK_0 = FK_param_Lf(PATH_LIST_full[0])\n",
    "    data_app = WB_FK_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST_full)):\n",
    "        WB_FK = FK_param_Lf(PATH_LIST_full[i_file])\n",
    "        data_app = np.concatenate((data_app,WB_FK),axis=0)\n",
    "    return data_app\n",
    "\n",
    "\n",
    "\n",
    "def Bod_param_Lf(PATH,time_window=15,coarsen_factor=12):\n",
    "    # FK08 parameterization inputs\n",
    "    tau = 86400\n",
    "    FCOR = smooth(xr.open_dataarray(PATH+'FCOR.nc'),time_window,coarsen_factor)\n",
    "    B_x = smooth(xr.open_dataarray(PATH+'B_x.nc'),time_window,coarsen_factor).values\n",
    "    B_y = smooth(xr.open_dataarray(PATH+'B_y.nc'),time_window,coarsen_factor).values\n",
    "    grad_B = np.sqrt(B_y**2 + B_x**2)\n",
    "    \n",
    "    HML = smooth(xr.open_dataarray(PATH+'HML.nc'),time_window,coarsen_factor).values\n",
    "    TAUX = smooth(xr.open_dataarray(PATH+'TAUX.nc'),time_window,coarsen_factor).values\n",
    "    TAUY = smooth(xr.open_dataarray(PATH+'TAUY.nc'),time_window,coarsen_factor).values\n",
    "    TAU = np.sqrt(TAUY**2 + TAUX**2)\n",
    "\n",
    "    Q = smooth(xr.open_dataarray(PATH+'Q.nc'),time_window,coarsen_factor).values\n",
    "    HBL = smooth(xr.open_dataarray(PATH+'HBL.nc'),time_window,coarsen_factor).values\n",
    "    \n",
    "    # WB FK11 with Bod Lf rescaling factor\n",
    "    ds = 25e3 # approx 1/4 degree resolution  ~ 25km\n",
    "    m_star = 0.5\n",
    "    n_star = 0.066\n",
    "    Cl = 0.25\n",
    "    u_star = np.sqrt(np.abs(TAU)/1000)\n",
    "\n",
    "    \n",
    "    Lf = (Cl * m_star * u_star**2)/(np.sqrt(FCOR**2 + tau**-2)*HBL)\n",
    "\n",
    "    WB_BD_Lf = (ds /Lf)*((HML**2) * (grad_B**2))/np.sqrt(FCOR**2 + tau**-2)\n",
    "    return WB_BD_Lf\n",
    "    \n",
    "    \n",
    "def load_data_Bod_Lf(time_window=15,coarsen_factor=12):\n",
    "    PATH_LIST_full = glob.glob(BASE+'*_smooth/preprcossed_data/') \n",
    "    WB_Bod_0 = Bod_param_Lf(PATH_LIST_full[0])\n",
    "    data_app = WB_Bod_0\n",
    "    for i_file in np.arange(1,len(PATH_LIST_full)):\n",
    "        WB_Bod = Bod_param_Lf(PATH_LIST_full[i_file])\n",
    "        data_app = np.concatenate((data_app,WB_Bod),axis=0)\n",
    "    return data_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WB subgrid\n",
    "WB_sg, WB_sg_mean, WB_sg_std = load_data_WB()\n",
    "\n",
    "np.save(PATH_NN+'WB_sg.npy',WB_sg)\n",
    "np.save(PATH_NN+'WB_sg_mean.npy',WB_sg_mean)\n",
    "np.save(PATH_NN+'WB_sg_std.npy',WB_sg_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WB FK parameterization\n",
    "WB_FK = load_data_FK()\n",
    "WB_FK_Lf = load_data_FK_Lf()\n",
    "WB_BD_Lf = load_data_Bod_Lf()\n",
    "\n",
    "np.save(PATH_NN+'WB_FK_Lf.npy',WB_FK_Lf)\n",
    "np.save(PATH_NN+'WB_BD_Lf.npy',WB_BD_Lf)\n",
    "np.save(PATH_NN+'WB_FK.npy',WB_FK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load FK08 variables relevant for w'b' parameterization:\n",
    "FCOR, FCOR_mean, FCOR_std = load_data_norm('FCOR') \n",
    "B_x, B_x_mean, B_x_std = load_data_norm('B_x') \n",
    "B_y, B_y_mean, B_y_std = load_data_norm('B_y')\n",
    "grad_B = np.sqrt(B_y**2 + B_x**2)\n",
    "\n",
    "HML, HML_mean, HML_std = load_data_norm('HML') \n",
    "Nsquared, Nsquared_mean, Nsquared_std = load_data_norm('Nsquared')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load other variables that may be relevant for w'b':\n",
    "\n",
    "U, U_mean, U_std = load_data_norm('U') \n",
    "V, V_mean, V_std = load_data_norm('V') \n",
    "\n",
    "TAUX, TAUX_mean, TAUX_std = load_data_norm('TAUX')\n",
    "TAUY, TAUY_mean, TAUY_std = load_data_norm('TAUY')\n",
    "TAU = np.sqrt(TAUY**2 + TAUX**2)\n",
    "\n",
    "Q, Q_mean, Q_std = load_data_norm('Q')\n",
    "HBL, HBL_mean, HBL_std = load_data_norm('HBL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save normalized NN data\n",
    "\n",
    "np.save(PATH_NN+'grad_B.npy',grad_B)\n",
    "np.save(PATH_NN+'FCOR.npy',FCOR)\n",
    "#np.save(PATH_NN+'WB_sg.npy',WB_sg)\n",
    "#np.save(PATH_NN+'WB_FK.npy',WB_FK)\n",
    "np.save(PATH_NN+'HML.npy',HML)\n",
    "\n",
    "np.save(PATH_NN+'TAU.npy',TAU)\n",
    "np.save(PATH_NN+'U.npy',U)\n",
    "np.save(PATH_NN+'V.npy',V)\n",
    "np.save(PATH_NN+'Q.npy',Q)\n",
    "np.save(PATH_NN+'HBL.npy',HBL)\n",
    "np.save(PATH_NN+'FCOR.nc',FCOR)\n",
    "\n",
    "#np.save(PATH_NN+'WB_sg.npy',WB_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "submeso_env",
   "language": "python",
   "name": "submeso_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
