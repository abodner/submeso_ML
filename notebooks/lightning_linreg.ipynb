{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabigail-bodner\u001b[0m (\u001b[33mabodner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/scratch/ab10313/pleiades/'\n",
    "PATH_NN= BASE+'NN_data_smooth/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import systems.regression_system as regression_system\n",
    "import models.linreg as linreg\n",
    "#import lightning.util.performance as performance\n",
    "#import util.misc as misc\n",
    "#import pyqg_explorer.dataset.forcing_dataset as forcing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X input shape:\n",
      "(8450, 40, 40, 10)\n",
      "\n",
      "Y output shape:\n",
      "(8450, 40, 40, 1)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load preprocessed data into input and output channels\n",
    "\n",
    "# X INPUT\n",
    "grad_B = np.load(PATH_NN+'grad_B.npy')\n",
    "FCOR = np.load(PATH_NN+'FCOR.npy')\n",
    "Nsquared = np.load(PATH_NN+'Nsquared.npy')\n",
    "HML = np.load(PATH_NN+'HML.npy')\n",
    "TAU = np.load(PATH_NN+'TAU.npy')\n",
    "Q = np.load(PATH_NN+'Q.npy')\n",
    "HBL = np.load(PATH_NN+'HBL.npy')\n",
    "div = np.load(PATH_NN+'div.npy')\n",
    "vort = np.load(PATH_NN+'vort.npy')\n",
    "strain = np.load(PATH_NN+'strain.npy')\n",
    "\n",
    "# note different reshaping of input/output for ANN\n",
    "X_input = np.stack([FCOR, grad_B, HML, Nsquared, TAU, Q, HBL, div, vort, strain],axis=0).reshape(grad_B.shape[0],grad_B.shape[1],grad_B.shape[2],10) \n",
    "print('X input shape:')\n",
    "print( X_input.shape)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Y OUTPUT\n",
    "WB_sg = np.load(PATH_NN+'WB_sg.npy')\n",
    "WB_sg_mean = np.load(PATH_NN+'WB_sg_mean.npy')\n",
    "WB_sg_std = np.load(PATH_NN+'WB_sg_std.npy')\n",
    "              \n",
    "Y_output = np.tile(WB_sg,(1,1,1,1)).reshape(WB_sg.shape[0],WB_sg.shape[1],WB_sg.shape[2],1) \n",
    "print('Y output shape:')\n",
    "print(Y_output.shape)\n",
    "print('')\n",
    "\n",
    "np.isnan(X_input).any()\n",
    "np.isnan(Y_output).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train 90.0%, test 10.0%\n",
      "no overlapping indecies\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST ONLY\n",
    "# randomnly generate train, test and validation time indecies \n",
    "import random\n",
    "time_ind = X_input.shape[0]\n",
    "rand_ind = np.arange(time_ind)\n",
    "rand_seed = 14\n",
    "random.Random(rand_seed).shuffle(rand_ind)\n",
    "train_percent = 0.9\n",
    "test_percent = 0.1 \n",
    "print(f\"Dataset: train {np.round(train_percent*100)}%, test {np.round(test_percent*100)}%\")\n",
    "train_ind, test_ind =  rand_ind[:round(train_percent*time_ind)], rand_ind[round((train_percent)*time_ind):]                                                                        \n",
    "\n",
    "# check no overlapping indecies\n",
    "if np.intersect1d(train_ind, test_ind).any():\n",
    "    print('overlapping indecies')\n",
    "else:\n",
    "    print ('no overlapping indecies')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "X input shape:\n",
      "torch.Size([12168000, 10])\n",
      "Y output shape:\n",
      "torch.Size([12168000, 1])\n",
      "\n",
      "TEST\n",
      "X input shape:\n",
      "torch.Size([1352000, 10])\n",
      "Y output shape:\n",
      "torch.Size([1352000, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define X,Y pairs (state, subgrid fluxes) for local network.local_torch_dataset = Data.TensorDataset(\n",
    "BATCH_SIZE = 64  # Number of sample in each batch\n",
    "\n",
    "\n",
    "###### training dataset #######\n",
    "torch_dataset_train = Data.TensorDataset(\n",
    "    torch.flatten(torch.from_numpy(X_input[train_ind]).float(), start_dim=0, end_dim=2) ,\n",
    "    torch.flatten(torch.from_numpy(Y_output[train_ind]).float(), start_dim=0, end_dim=2) ,\n",
    ")\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=1\n",
    ")\n",
    "print('TRAIN')\n",
    "print('X input shape:')\n",
    "print(torch.flatten(torch.from_numpy(X_input[train_ind]).float(), start_dim=0, end_dim=2).shape)\n",
    "print('Y output shape:')\n",
    "print( torch.flatten(torch.from_numpy(Y_output[train_ind]).float(), start_dim=0, end_dim=2).shape)\n",
    "print('')\n",
    "\n",
    "###### test dataset #######\n",
    "torch_dataset_test = Data.TensorDataset(\n",
    "    torch.flatten(torch.from_numpy(X_input[test_ind]).float(), start_dim=0, end_dim=2),\n",
    "    torch.flatten(torch.from_numpy(Y_output[test_ind]).float(), start_dim=0, end_dim=2)    \n",
    ")\n",
    "\n",
    "BATCH_SIZE_TEST = len(torch_dataset_test)\n",
    "\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=torch_dataset_test, batch_size=BATCH_SIZE_TEST, shuffle=False, num_workers=1\n",
    ")\n",
    "\n",
    "print('TEST')\n",
    "print('X input shape:')\n",
    "print(torch.flatten(torch.from_numpy(X_input[test_ind]).float(), start_dim=0, end_dim=2).shape)\n",
    "print('Y output shape:')\n",
    "print( torch.flatten(torch.from_numpy(Y_output[test_ind]).float(), start_dim=0, end_dim=2).shape)\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use GPUs if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print('CUDA Not Available')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123\n",
    "batch_size=64\n",
    "input_size=10\n",
    "output_size=1\n",
    "hidden_size = [10,10,10]\n",
    "activation=\"ReLU\"\n",
    "arch=\"linreg\"\n",
    "epochs=12\n",
    "nz=24\n",
    "save_path=BASE+\"models/\"\n",
    "save_name=\"linreg.pt\"\n",
    "lr=0.0001\n",
    "wd=0.01\n",
    "\n",
    "## Wandb config file\n",
    "config={\"seed\":seed,\n",
    "        \"lr\":lr,\n",
    "        \"wd\":wd,\n",
    "        \"batch_size\":batch_size,\n",
    "        \"input_size\":input_size,\n",
    "        \"output_size\":output_size,\n",
    "        \"activation\":activation,\n",
    "        \"save_name\":save_name,\n",
    "        \"save_path\":save_path,\n",
    "        \"arch\":arch,\n",
    "        \"hidden_size\":hidden_size,\n",
    "        \"epochs\":epochs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xyqvf4xa) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-aardvark-269</strong> at: <a href='https://wandb.ai/abodner/submeso_ML/runs/xyqvf4xa' target=\"_blank\">https://wandb.ai/abodner/submeso_ML/runs/xyqvf4xa</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230627_144910-xyqvf4xa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xyqvf4xa). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861369bb34b34b1e8b81b5f4ab348086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668261385833223, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ab10313/submeso_ML/nn/lightning/wandb/run-20230627_144942-oadh4w6r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abodner/submeso_ML/runs/oadh4w6r' target=\"_blank\">ethereal-tree-270</a></strong> to <a href='https://wandb.ai/abodner/submeso_ML' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abodner/submeso_ML' target=\"_blank\">https://wandb.ai/abodner/submeso_ML</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abodner/submeso_ML/runs/oadh4w6r' target=\"_blank\">https://wandb.ai/abodner/submeso_ML/runs/oadh4w6r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=linreg.LinearRegression(config)\n",
    "system=regression_system.RegressionSystem(model)\n",
    "\n",
    "## Store the number of learanble parameters\n",
    "config[\"learnable parameters\"]=sum(p.numel() for p in model.parameters())\n",
    "\n",
    "## Initialise wandb run - pass config dictionary storing the model parameters\n",
    "wandb.init(project=\"submeso_ML\",config=config)\n",
    "wandb.watch(model, log_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/ab10313/pleiades/models/linreg.pt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path+save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab10313/.local/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ab10313/.local/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | network   | LinearRegression | 11    \n",
      "1 | criterion | MSELoss          | 0     \n",
      "-----------------------------------------------\n",
      "11        Trainable params\n",
      "0         Non-trainable params\n",
      "11        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ab10313/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ab10313/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14141a26e8774fe78f8ec1b8a29f254f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    logger=WandbLogger()\n",
    ")\n",
    "\n",
    "trainer.fit(system, train_loader, test_loader)\n",
    "#wandb.log({\"loss\": loss,\"epoch\": epoch,\"R-squared\": r2, \"correlation\": corr })\n",
    "\n",
    "model.save_model(save_path+save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
